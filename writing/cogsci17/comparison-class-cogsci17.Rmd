---
title: "Warm (for winter): Comparison class understanding in vague language"
bibliography: [comparison-class.bib, library.bib]
csl: "apa6.csl"
document-params: "10pt, letterpaper"
header-includes:
  - \usepackage{tabularx}
  - \usepackage{multicol}
  - \usepackage{wrapfig}
  - \usepackage{gensymb}
  - \usepackage{tikz}
  - \usepackage{caption}
  #- \usepackage{stfloats}

author-information: >
  \author{{\large \bf Michael Henry Tessler$^1$} (mtessler@stanford.edu) \and {\large \bf Michael Lopez-Brau$^2$} (lopez\_mic@knights.ucf.edu) \\
  {\large \bf Noah D. Goodman$^1$} (ngoodman@stanford.edu) \\
  $^1$Department of Psychology, Stanford University,
  $^2$Department of Electrical \& Computer Engineering, University of Central Florida}

abstract:
    "Speakers implicitly refer to categories when predicating properties of objects or entities.
    *It's warm outside* could mean it's warm relative to other days of the year, or just relative to the current season (e.g., it's warm for winter).
    *Warm* vaguely conveys that the temperature is high relative to some *comparison class*, but little is known about how a listener decides upon such a standard of comparison. 
    Here, we propose the resolution of a comparison class in context is a pragmatic inference driven by world knowledge and listeners' internal models of speech production.
    We introduce a Rational Speech Act model and derive two novel predictions from the model, which we validate using a paraphrase experiment to measure listeners' beliefs about the likely comparison class used by a speaker.
    We generate quantitative model predictions by incorporating our model into Bayesian data analytic framework, and infer the likely priors by collecting further language understanding data in the same domains. 
    This represents a new possibility for Bayesian models of cognition and language: by building more complete and coherent models of language, additional parameters can be estimated reliably by asking the model to say different things using the same domain knowledge.
    "

keywords:
    "comparison class; pragmatics; Bayesian cognitive model; Bayesian data analysis"

output: cogsci2016::cogsci_paper
---

\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}
\definecolor{Orange}{RGB}{255,153,0}

\newcommand{\red}[1]{\textcolor{Red}{#1}}  
\newcommand{\ndg}[1]{\textcolor{Green}{[ndg: #1]}}  
\newcommand{\mht}[1]{\textcolor{Blue}{[mht: #1]}}  
\newcommand{\mlb}[1]{\textcolor{Orange}{[mlb: #1]}}

```{r global_options, include=FALSE}
rm(list=ls())

# set local path for the repo
if (grepl("michael", getwd())) {
  project.path <- "/media/michael/Data/Desktop/comparison-class/"
} else if (grepl("mht", getwd())) {
  project.path <- "/Users/mht/Documents/research/comparison-class/"
} else {
  project.path <- "/Users/ngoodman/comparison-class/"
}

knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop=F, fig.pos="tb", fig.path='figs/', echo=F, warning=F, cache=F, message=F, sanitize=T)

```

```{r, libraries}
library(png)
library(grid)
library(gridExtra)
library(tidyverse)
library(xtable)
library(rwebppl)
library(langcog)
library(coda)
library(lme4)
library(lmerTest)
library(data.table)
#install.packages("ggthemes")
library(ggthemes)
library(RColorBrewer)
estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}

HPDhi<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

HPDlo<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}
```

```{r}
theme_set(theme_minimal(9))
```

<!-- 
TO DO: 
[ ] unify treatment of subordinate / superordinate vs. specific / general... which is clearer? [ ] have lassiter RSA model appear on page 1 (intro should be condensed)

NOTES:
- using $cc$ as the var for comparison makes makes Eq. (4) go onto 2 lines
-->

If it's 75\degree F (24\degree C) outside, you could say "it's warm".
But if it's 60\degree F (16\degree C), you might not consider it warm.
Unless it's January; it could be warm for January.
*Warm* is relative, and its felicitous application depends upon what the speaker uses as a basis of comparison---the *comparison class* (e.g., other days of the year or other days in January).
When you say "It's warm outside", the comparison class goes unsaid.
<!--
If someone tells you it's warm outside, it's still useful to remember if it's July or January.
A warm day in July could be 80\degree F while a warm one in January might not get above 60\degree F.
The predicate "warm" is vague in the sense that there are borderline cases of being warm--interlocutors can faultlessly disagree as to whether or not it is warm since what it means to be warm is relative to a relevant *comparison class*.
-->
<!--\ndg{opening line doesn't make much sense. start with a vague utterance, point out that whether it's true and/or its interpretation depends on comparison class.}-->
<!--A warm day in July could be 80\degree F while a warm one in January might not get above 60\degree F.-->

The existence of reference classes in the study of vague language is uncontroversial [@Solt2009; @Bale2011]. 
4 year-olds categorize novel creatures (*pimwits*) as either *tall* or *short* depending on the distribution of heights of pimwits; the comparison class here is other pimwits, because children's judgments are unaffected by the heights of creatures that aren't pimwits [@Barner2008].
Adult judgments of the felicity for adjectives like "dark" or "tall" similarly depend upon fine-grained details of the statistics of the comparison class [@Schmidt2009; @Solt2012; @Qing2014].

<!--
Even 4 year-olds understand that what counts as "tall" or "short" depends upon the comparison class. 
2- or 3-year-olds understand that the meaning of the words "big" and "little" depend upon the context: 
An object can be "big" relative to other objects in a scene (i.e., its physical context), 
an object can be "big" relative to the size it should be for completing its intended function, 
or an object can just be "big" [i.e., big relative to an implicit comparison class; @Ebeling1994]. 
-->

Less well understood, however, is how a comparison class becomes contextually-salient. 
The problem is an obvious one because any particular object of discourse can be conceptualized or categorized in multiple ways, giving rise to multiple possible comparison classes. 
A day in January is also a day of the year; if it's warm, it could be *warm for Winter* or *warm relative to other days of the year*. 
<!--If someone tells you it's warm outside, that could mean it's just warm (i.e., *warm relative to the other days of the year*) or it could mean it's warm for this time of year (e.g., *warm for Winter*).-->
The issue of the comparison class touches upon not only adjectives, but in fact any language in which knowledge of the context is critical to the meaning, including vague quantifiers [e.g., "He ate a lot of burgers."; @SchollerFranke2015] and generic language [e.g., "Dogs are friendly."; @Tessler2016].

We propose that listeners actively use category knowledge and knowledge about what classes are likely to be talked about to infer the reference class  being used by the speaker.
<!--We propose that vague language gets interpreted with respect to a particular comparison class via a pragmatic reasoning process.-->
We introduce a minimal extension to the Rational Speech Act (RSA) model for gradable adjectives [@Lassiter2013] to allow it to flexibly reason about the likely comparison class.
From this model, we derive two novel predictions:
(1) *Ceteris paribus*, a listener should prefer reference classes that are more specific. A more specific class will have relatively lower variance, which gives a vague utterance a more specific meaning. 
(2) The likely comparison class will be modulated by the *a priori* plausibility that the adjective could apply to a member of specific class (e.g., how likely a day in Winter is to be warm relative to other days of the year). 
This second prediction in turn depends upon the quantitative relationship of the specific to the general class (e.g., is the specific class expected to be high or low relative to the general class?) and the form of the adjective (positive or negative e.g., warm or cold). 
For instance, in Winter, the positive form adjective phrase "It's warm" should signal the more specific class (relative to days in Winter) more so than the negative form "It's cold". 
We test these predictions in an experiment designed to elicit the comparison class (Expt. 1).

The RSA model is a quantitative model though it's predictions rely upon the fine-grained details of the relevant world knowledge.
This background knowledge can often be measured by having participants estimate quantities or give likelihood judgments in the same contexts without experimental manipulation [@FrankeEtAl2016].
We pursue a different methodology.
We explicitly put uncertainty over the parameters of the background knowledge being used in this task, and use Bayesian data analysis to fill in the likely values.
This approach by itself has too many parameters to estimate reliably, but the we can use the same RSA model to help estimate these parameters by predicting further language data in which the same prior knowledge is used.
Modeling productive language use is a method for productive experiment design, which we use to pin down necessary model parameters.
<!--
, though, and we use it to predict further language data to pin down these needed parameters.
This represents a new possibility for Bayesian models of language: by building more complete and coherent models of language understanding, additional parameters can be estimated reliably by asking the model to answer multiple questions about the same domain knowledge.
he same model with the same prior knowledge is used to predict data from both experiments.
Thus, we use our same RSA model to predict data for similar kinds of language tasks in a follow-up experiment; critically, t
"It's warm outside" when heard in July means something very different than when heard in January.
Warm is vague,
When heard in Winter, "It's warm" might mean it's 50 degrees; in Summer, maybe 80.
-->

# Understanding comparison classes

Adjectives like *warm* and *cold* are vague descriptions of an underlying quantitative scale (e.g., temperature).
The vagueness and context-sensitivity of these adjectival utterances $u$ can be modeled using a threshold semantics ($[[u]] = x > \theta$) where the threshold $\theta$ comes from an uninformed prior distribution and is inferred in context via pragmatic reasoning [@Lassiter2013; see also @Qing2014a].
<!---->
\begin{align}
L_{1}(x, \theta \mid u) &\propto S_{1}(u \mid x, \theta) \cdot P_{c}(x) \cdot P(\theta) \label{eq:L1} \\
S_{1}(u \mid x, \theta) &\propto \exp{(\alpha_1 \cdot \ln {L_{0}(x \mid u, \theta)})} \label{eq:S1}\\
L_{0}(x \mid u, \theta) &\propto {\delta_{[[u]](x, \theta)} \cdot P_{c}(x)}. \label{eq:L0}
\end{align}
<!---->
This is a Rational Speech Act (RSA) model, a recursive Bayesian model where speaker $S$ and listener $L$ coordinate on an intended meaning [for a review, see @Goodman2016].
In this framework, the pragmatic listener $L_1$ tries to resolve the state of the world $x$ (e.g., the temperature) from the utterance she heard $u$ (e.g., "It's warm").
She assumes the utterance came from an approximately rational Bayesian speaker $S_1$ who tried to inform a naive listener $L_0$, who in turn updates her prior beliefs $P(x)$ via an utterance's literal meaning $[[u]](x)$.
@Lassiter2013 introduced uncertainty over a semantic variable, the truth-functional threshold $\theta$ (Eq. \ref{eq:L1}).
The uncertainty over $\theta$ (e.g., the point at which something is *warm*) interacts with the prior distribution over possible states of the world $P_{c}(x)$ (e.g., possible temperatures) and the recursive pragmatic reasoning of the model to resolve the meaning of the adjective in context.
What is made clear from this model is that the prior distribution over world-states is always relative to some comparison class $c$ (Eqs. \ref{eq:L1} \& \ref{eq:L0}).
But where does such a comparison class come from?

<!--\ndg{next few paragraphs can be tightened up a bit. maybe combine next two?}-->

When a listener hears only that "It's warm outside" without an explicit comparison class (e.g., "warm for the season"), we imagine the listener must infer the comparison class by thinking about what would best complete the sentence, in a way analagous to noisy-channel models of production and comprehension [@Bergen2015].
She does this using her world knowledge of what worlds are plausible given different comparison classes $P(x \mid c)$, what comparison classes are likely to be talked about $P(c)$, and what a rational speaker would say in given a world and comparison class $S_{1}(u \mid x, c, \theta)$ (Eq. \ref{eq:L1a}).
<!--We elaborate the Rational Speech Act theory for adjective understanding [@Lassiter2013; @Lassiter2015] by introducing uncertainty over the class of entities against which the target entity is compared --- the comparison class $c$ .-->
As a first test of this idea, we consider an idealized case where the comparison class can be either a relatively specific (subordinate) or relatively general (superordinate) categorization (e.g., warm relative to days in Winter or relative to days of the year). 
The listener is aware that the target entity is in the subordinate class (e.g., aware that it is winter) and draws likely values of the degree (e.g., temperature) from the subordinate class prior: $P(x \mid c_{sub})$.
<!---->
\begin{align}
L_{1}(x, c, \theta \mid u) &\propto S_{1}(u \mid x, c, \theta) \cdot P(x \mid c_{sub}) \cdot P(c) \cdot P(\theta) \label{eq:L1a}\\
S_{1}(u \mid x, c, \theta) &\propto \exp{(\alpha_1 \cdot \ln {L_{0}(x \mid u, c, \theta)})} \label{eq:S1a}\\
L_{0}(x \mid u, c, \theta) &\propto {\delta_{[[u]](x, \theta)} \cdot P(x \mid c)}. \label{eq:L0a}
\end{align}
<!---->
We use a speaker model that considers the alternatives of using the same utterance with the explicit comparison class (e.g., "It is warm relative to other days in Winter." or "It is warm relative to other days of the year."), for which we assume equal production cost for simplicity. 
<!--For simplicity of the model, we assume each utterance has equal cost.-->
We are interested in the behavior of the model with the underspecified utterance (e.g., "It is warm").


```{r wpplHelpers}
webpplHelpers <- '
var round = function(x){
  return Math.round(x*10)/10
}

var distProbs = function(dist, supp) {
  return map(function(s) {
    return Math.exp(dist.score(s))
  }, supp)
}

var KL = function(p, q, supp) {
  var P = distProbs(p, supp), Q = distProbs(q, supp);
  var diverge = function(xp,xq) {
    return xp == 0 ? 0 : (xp * Math.log(xp / xq) );
  };
  return sum(map2(diverge,P,Q));
};

var exp = function(x){return Math.exp(x)}
'
```

```{r rsaPrior}
priorForRSA <- '
var binParam = 4;

var stateParams = {
  sub: paramsFromR.priorParams.sub[0],
  super: paramsFromR.priorParams.super[0]
};

var stateVals = map(
  round,
  _.range(stateParams.super.mu - 3 * stateParams.super.sigma,
          stateParams.super.mu + 3 * stateParams.super.sigma,
          stateParams.super.sigma/binParam)
);

var stateProbs = {
  sub: map(function(s){
    Math.exp(Gaussian(stateParams.sub).score(s))+
    Number.EPSILON
  }, stateVals),
  super: map(function(s){
    Math.exp(Gaussian(stateParams.super).score(s))+
    Number.EPSILON
  }, stateVals)
};

var statePrior = {
  sub: Infer({
    model: function(){ return categorical({vs: stateVals, ps: stateProbs.sub}) }
  }),
  super: Infer({
    model: function(){ return categorical({ vs: stateVals, ps: stateProbs.super}) }
  })
};
'
```

```{r rsaLanguage}
languageForRSA <- '
var thresholdBins ={
  positive: map(function(x){
    return  x - (1/(binParam*2));
  }, sort(statePrior.super.support())),
  negative: map(function(x){
    return  x + (1/(binParam*2));
  }, sort(statePrior.super.support()))
};

var thresholdPrior = cache(function(form){
  return Infer({
    model: function() { return uniformDraw(thresholdBins[form]) }
  });
});

var utterances = {
  positive: ["positive_Adjective",
             "positive_sub",
             "positive_super"],
  negative: ["negative_Adjective",
             "negative_sub",
             "negative_super"]
};

var utteranceProbs = [1, 1, 1];
var utterancePrior = cache(function(form){
  return Infer({
    model: function() {
      return categorical({
        vs: utterances[form],
        ps: utteranceProbs
      })
    }
  })
});

var meaning = function(utterance, state, thresholds) {
  utterance == "positive" ? state > thresholds.positive ? flip(0.9999) : flip(0.0001) :
  utterance == "negative" ? state < thresholds.negative ? flip(0.9999) : flip(0.0001) :
  true
}

'
```

```{r ccRSA}
ccrsa <- '
// webppl ccrsa.wppl --require adjectiveRSA
var classPrior = Infer({
  model: function(){return uniformDraw(["sub", "super"])}
});

var alphas = {s1: 3, s2: 1};

var literalListener = cache(function(u, thresholds, comparisonClass) {
  Infer({model: function(){
    var cc = u.split("_")[1] == "Adjective" ?
        comparisonClass :
    u.split("_")[1] == "silence" ?
        comparisonClass :
    u.split("_")[1]    

    var state = sample(statePrior[cc]);
    var utterance = u.split("_")[0]
    var m = meaning(utterance, state, thresholds);
    condition(m);
    return state;
  }})
}, 10000)

var speaker1 = cache(function(state, thresholds, comparisonClass, form) {
  Infer({model: function(){
    var utterance = sample(utterancePrior(form))
    var L0 = literalListener(utterance, thresholds, comparisonClass)
    factor( alphas.s1 * L0.score(state) )
    return utterance
  }})
}, 10000)

var pragmaticListener = function(form) {
  Infer({model: function(){
    var utterance = form + "_Adjective";
    var comparisonClass = sample(classPrior);
    var state = sample(statePrior["sub"]);
    var thresholds = form == "positive" ? {
      positive: sample(thresholdPrior("positive"))
    } : {
      negative: sample(thresholdPrior("negative"))
    }
    var S1 = speaker1(state, thresholds, comparisonClass, form);
    observe(S1, utterance);
    return comparisonClass
  }})
}

pragmaticListener(paramsFromR.utt[0])
'
```

```{r runModel, cache = T}
sub.prior.params <- c( 
  list( sub = data.frame(mu = 1, sigma = 0.5) ),
   list( sub = data.frame(mu = 0, sigma = 0.5) ),
   list( sub = data.frame(mu = -1, sigma = 0.5) )
  )

mp.both <- data.frame()

fullModel <- paste(webpplHelpers, priorForRSA, languageForRSA, ccrsa, sep = "\n")

for (p in sub.prior.params){
  prior.params <- list(super = data.frame(mu = 0, sigma = 1), sub = p)

  for (u in c("positive", "negative")){
  
    mp <- webppl(
      program_code = fullModel,
      data = list(utt = u, priorParams = prior.params),
      data_var = "paramsFromR"
    )

    mp.both <- bind_rows(mp %>%
      filter(support == "super") %>%
      mutate(u = u, sub_mu = p["mu"][[1]], sub_sigma = p["sigma"][[1]]),
      mp.both)
  }
  #print(p$mu)
}

```

```{r priorsModel, cache = T}
all.priors <- bind_rows(
  data.frame(
    val = rnorm(10000, mean = 0, sd = 1),
    cat = "super"
    ),
  data.frame(
    val = rnorm(10000, mean = -1, sd = 0.5),
    cat = "low"
    ),
  data.frame(
    val = rnorm(10000, mean = 0, sd = 0.5),
    cat = "medium"
  ),
  data.frame(
    val = rnorm(10000, mean = 1, sd = 0.5),
    cat = "high"
  )
) %>% mutate(cat = factor(cat, levels = c("super", "low", "medium", "high")))
 
```


<!--
When the pragmatic listener hears this utterance, she must reason about which comparison class the speaker had in mind.
When the speaker produces the unambiguous alterantive utterance (e.g., "It is relative to other days in Winter."), the listener 
-->
<!--In the following section, we explore the predictions of this model.-->
<!--
We posit that this knowledge is the listener's internal model of speech production of different classes of entities, and thus this comparison class prior should include information about the frequency of different classes. 
\mht{mention "basic level" here?}
-->

```{r modelSchematics, fig.env = "figure", fig.pos = "htb", fig.width=3.3, fig.height=2.5, num.cols.cap=1, set.cap.width=T, fig.cap = "Left: Three hypothetical prior distributions over subordinate comparison class (fixing the superordinate comparison class to be a unit-normal distribution, in grey). Right: Predicted listener inference for the probability of an intended superordinate comparison class. The comparison class prior does not favor either the subordinate or superordinate class."}

subplt1 <- ggplot(all.priors, aes(x = val,fill = cat, 
                              lty = cat, group= cat, alpha=cat))+
  geom_density(adjust = 1.3)+
  xlab("Degree")+
  ylab("Probability density")+
  scale_fill_manual(values = c("#636363","#ffeda0","#feb24c","#f03b20"),
                    breaks = c("low", "medium", "high")) +
  scale_alpha_manual(values = c(1,0.6,0.6,0.6),
                    breaks = c("low", "medium", "high"))+
  scale_linetype_manual(values = c(1,3,2,1),
                    breaks = c("low", "medium", "high"))+
  scale_x_continuous(limits = c(-3, 3), breaks = c(-2, 0, 2))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 1))+
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())


subplt2 <- ggplot(mp.both %>%
                    mutate(Adjective = factor(u,
                                      levels = c("negative", "positive")),
                           sub_mu = factor(sub_mu, levels = c(-1, 0, 1),
                                           labels = c("low", "medium",
                                                      "high"))),
                  aes(x = sub_mu, y = prob, fill = Adjective))+
  geom_bar(stat= 'identity', 
           position = position_dodge(), 
           color = 'black', 
           alpha = 0.8, width = 0.75)+
  scale_fill_brewer(palette = "Set3")+
  geom_hline(yintercept = 0.5, lty = 3)+
  xlab("Subordinate prior mean")+
  ylab("Superordinate interpretation")+
  scale_y_continuous(limits = c(0, 0.75), breaks = c(0, 0.5))+
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank())
  


grid.arrange(subplt1, subplt2, nrow = 1)
```

```{r data_collection_plan_simulation, eval = F, echo=F}
## simulations to determine width of 95% CI for 2AFC data assuming different sample sizes and true binomial probabilities

### not written efficiently... will take ~10 minutes to run
n_participants <- c(50, 75, 100)
true_probs <- c(0.1, 0.3, 0.5)
simulations <- data.frame()
for (n in n_participants){
  for (p in true_probs){
    for (i in seq(1, 25)){
      simulations <- bind_rows(simulations,
            bind_rows(data.frame(label = c("a"),
                       response = rbinom(n =n, size = 1, prob = p)),
                data.frame(label = c("b"),
                           response = rbinom(n =n, size = 1, prob = p))) %>%
            group_by(label) %>%
            multi_boot_standard(column = "response") %>%
            mutate(width = ci_upper - ci_lower) %>%
          ungroup() %>%
          summarize(w = mean(width)) %>%
            mutate(n = n, p = p, i=i)
      )
    }
  }
}

ggplot(simulations, aes(x = w))+
 geom_histogram()+
 facet_grid(n~p)

```

```{r loadExpt1Data, cache=T}
data.path <- paste(project.path, "data/classElicitation-1/", sep = "")

d.catch <- read.csv(paste(data.path, "class-elicitation-full-catch_trials.csv", sep = ""))
d.catch <- d.catch %>%
  mutate(pass = response == "relative to other buildings") %>%
  select(workerid, pass)

d <- read.csv(paste(data.path, "class-elicitation-full-trials.csv", sep = ""))
d.tidy <- left_join(d, d.catch) %>%
  filter(pass) %>%
  mutate(superResponse = ifelse(paraphrase == "super", 1, 0))

# df.summary <- d.tidy %>%
#   group_by(strength, target, degree, adjective, form, sub_category, super_category) %>%
# #  group_by(condition, strength, target, degree, adjective, form, sub_category, super_category) %>%
#   multi_boot_standard(column = "superResponse")

df.bayes <- d %>%
  mutate(superResponse = ifelse(paraphrase == "super", 1, 0)) %>%
  group_by(strength, target, degree, adjective, form, sub_category, super_category) %>%
  summarize(k = sum(superResponse), n = n()) %>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         low  = qbeta(.025, a, b),
         high = qbeta(.975, a, b),
         MAP_h = (a-1)/(a+b-2),
         mean = a / (a + b))

```


```{r loadExpt2Data, cache = T}
project.name <- "vague-prior-elicitation-1"
data.path <- paste(project.path, "data/vagueSpeaker-1/", sep = "")
d.catch <- read.csv(paste(data.path, project.name, "-catch_trials.csv", sep = ""))
d.catch <- d.catch %>% 
  mutate(pass = response == "Yes")

d <- read.csv(paste(data.path, project.name, "-trials.csv", sep = ""))

# vs.summary <- left_join(d, d.catch %>% select(workerid, pass)) %>%
#   filter(pass) %>%
#   group_by(strength, target, degree, adjective, 
#            form, sub_category, super_category) %>%
#   multi_boot_standard(column = "response")

vs.bayes <- left_join(d, d.catch %>% select(workerid, pass)) %>%
  filter(pass) %>%
  group_by(strength, target, degree, adjective, 
           form, sub_category, super_category) %>%
  summarize(k = sum(response), n = n()) %>%
  ungroup() %>%
  mutate(a = 1 + k,
         b = 1 + n - k,
         low  = qbeta(.025, a, b),
         high = qbeta(.975, a, b),
         MAP_h = (a-1)/(a+b-2),
         mean = a / (a + b))

```



## Model predictions

As a first test of this pragmatic theory, we consider a simple case:
The comparison class is either a subordinate level category (e.g., days in Winter) or a superordinate level category (e.g., days of the year).
For purposes of illustration, we assume for now each class is equally likely *a priori*: $c \sim \text{UniformDraw}([c_{sub}, c_{super}])$ (this assumption will be relaxed in the next section).
We assume the prior distribution over the degree for the superordinate class is a unit-normal distribution $x_{super} \sim \mathcal{N}(0, 1)$ and explore the model predictions as we vary the mean for the prior of the subordinate class.
Throughout, we assume the listener $L_1$ knows the target entity is of the subordinate class (e.g., knows that it's Winter): $P(x \mid c_{sub})$, but that the speaker might have had a different comparison class in mind.


<!--
Model schematic. Show "John is tall for a person", "John is tall for a baskebatll player", "John is tall."
-->
Consider the subordinate priors that correspond to temperatures in Winter (low), Fall (medium), and Summer (high) in a part of the world that exhibits seasons (e.g., Maryland, USA); the superordinate prior will correspond to temperatures over the whole year (Figure \ref{fig:modelSchematics}, left).
All subordinate priors have lower variance ($\sigma = 0.5$) than the superordinate ($\sigma = 1$).
We see that regardless of the adjective form (positive or negative) or the mean of the subordinate prior (low, medium, high), the pragmatic listener (Eq. \ref{eq:L1a}) prefers the more specific comparison class (i.e., the subordinate class; Figure \ref{fig:modelSchematics}, right; all bars below baseline).
<!--This comes from the vagueness of adjectives: The meaning of the adjective in context depends upon the prior; a prior with less variance will result in a relatively more precise meaning.-->
We also see that during Winter, when the pragmatic listener hears "It's warm" (positive adjective), she thinks it's more likely that it's warm relative to days in Winter than when she hears it's cold (Figure \ref{fig:modelSchematics}, right, left-most bars).
The opposite is true in Summer: A warm day in Summer could be warm for Summer, or warm for the year, while a cold day in Summer is much more likely to be cold for Summer.
In sum, we see two predictions: The pragmatic listener overall prefers subordinate comparison classes, though the extent of this preference is modulated by the *a priori* probability that the adjective is true of the superordinate category.
We will test these two predictions in our first experiment.
<!--
We see that as the mean of the subordinate prior becomes greater than the superordinate prior, positive form adjectives (e.g., *tall*, *expensive*, *warm*) will be more likely to imply a superordinate comparison class (e.g., *people in general*), while negative form adjectives (e.g., *short*, *cheap*, *cold*) will imply a subordinate class (e.g., *basketball players*).
The opposite pattern is observed for subordinates priors with a mean substantially lower than the superordinate prior.
We test these predictions in our first experiment.
-->

## Prior knowledge

The RSA model makes quantitative predictions as well, though these depend on the quantitative details of the listener's knowledge of the subordinate and superordinate categories: $P(x \mid c_{sub})$ and $P(x \mid c_{super})$, as well as the prior distribution on comparison classes $P(c)$ in Eq. \ref{eq:L1a}.


### Comparison class prior

We posit that $P(c)$ reflects listeners' expectations about the likely completions of the (otherwise incomplete) adjective sentence (e.g., "It's warm relative to other $c$").
As a proxy for comparison class usage frequency, we use empirical frequency $\hat{f}$ estimated from the Google WebGram corpus^[
Corpus accessed via 
\url{http://corpora.linguistik.uni-erlangen.de/demos/cgi-bin/Web1T5/Web1T5_freq.perl}.
Due to the idiosyncracies of our items and potential polysemy in some of words, we made the following substitutions when querying the database for emprical frequency: produce $\rightarrow$ "fruits and vegetables"; things you watch online $\rightarrow$ "online videos"; days in \{season\} $\rightarrow$ "\{season\} days"; dishwashers $\rightarrow$ "dishwashing machines"; videos of cute animals $\rightarrow$ "animal videos".
], and scale it by a free parameter $\beta$: $P(c) \propto \exp{(\beta \cdot \log \hat{f})}$.

### Degree priors (world knowledge)

$P(x \mid c_{sub})$ and $P(x \mid c_{super})$ represent listeners' beliefs about the likely values of a degree (e.g., temperature) within a subordinate or superordinate class.
This kind of world knowledge can be measured by having participants estimate relevant quantities without experimental manipulation [e.g., the temperature in Maryland on a day in Winter; @FrankeEtAl2016].
In practice, these quantities can be difficult to estimate, especially for more abstract priors (e.g., the temperature in Maryland on any given day of the year).
Since the comparison class inference task we've described uses just two-alternatives, the numerical scale of the degree priors is irrelevant.
All that matters is the relationship between the subordinate and superordinate priors.

We fix each superordinate distribution to be a Unit-Normal $P(x \mid c_{super}) = \text{Gaussian}(0, 1)$ and the subordinate priors to also be Gaussian distributions; the subordinate priors thus have standardized units. 
We put priors over the mean $\mu$ and variance $\sigma$ of each subordinate Gaussian. 


## Reducing uncertainty with productive language use

We've articulated a Bayesian treatment of the relevant prior knowledge for comparison class inference.
This by itself, however, is not sufficient to fill in the probabilistic knowledge that the RSA model relies upon; the full model (RSA + Bayesian data analysis of the priors) has too many parameters to estimate reliably using only the data from a single comparison class experiment 
Even if they could be estimated, it's hard to know if the parameters we're uncovering are the true priors for participants' knowledge, or just extra parameters used to fit the data. 
This need not hold us back though. 
The RSA model is a productive language model: We can have it predict related language data to gain more traction on the latent world knowledge.

We choose an experiment that uses the same domain knowledge as used in a comparison class inference (Expt. 1) in order restrict the number of parameters in the model.
In addition to helping us gain traction on participants' prior knowledge, a second experiment can provide a further test for the RSA model. 
Thus, we design an experiment where participants are asked to use both subordinate and superordinate category knowledge about the degree, while avoiding asking them to estimate quantities. 
We re-use the vague adjective interpretation part of the RSA model to predict participants' endorsements of adjectives.

We add a level of recursion to the RSA model and use a pragmatic speaker to model adjective endorsement [Expt. 2; following @Qing2014a; @Tessler2016; @Tessler2016cogsci].
For this task, we remove comparison class uncertainty from the model, since the experiment makes use of explicit comparison classes.
<!---->
\begin{align}
S_{2}(u \mid c_{sub}) &\propto \exp{(\alpha_2 \cdot \ln{L_1(x, \theta \mid u, c_{super})})} \label{eq:S2} \\
L_{1}(x, \theta \mid u, c_{super}) &\propto S_{1}(u \mid x, c_{super}, \theta) \cdot P(x \mid c_{sub}) \cdot P(\theta) \label{eq:L1b}
\end{align}
<!---->
\mht{In this model, I use a KL-speaker (rather than the usual factor based on the true state of the world speaker). is that math different?}

We can then use data from both experiments to simulatenously reconstruct the prior knowledge used in the tasks and generate predictions for the two data sets.
<!-- The pragmatic listener model in Eq. \ref{eq:L1a} is a model of comparison class inference (Expt. 1). -->
<!-- We use Eq. \ref{eq:L1a} to model the comparison class inference task (Expt. 1). -->

```{r stim_table, echo = F, eval =F, results = "asis", fig.env = "table*", fig.pos = "b", fig.width = 4, fig.height = 2, fig.align = "center", set.cap.width = T, num.cols.cap = 2, fig.cap = "This image spans both columns. And the caption text is limited to 0.8 of the width of the document."}

height_sub = c("professional basketball player, professional gymnast, professional soccer player")
height_super = c("people")
price_sub = c("dishwasher, oven, toaster")
price_super = c("kitchen appliances")
temperature_sub = c("Fall in Maryland, Summer in Maryland, Winter in Maryland")
temperature_super = c("days of the year in Maryland")
time_sub = c("video of a cute animal, music video, movie")
time_super = c("things you watch online")
weight_sub = c("apple, grape, watermelon")
weight_super = c("produce")

scales = c("Height", "Price", "Temperature", "Time", "Weight")
sub = c(height_sub, price_sub, temperature_sub, time_sub, weight_sub)
super = c(height_super, price_super, temperature_super, time_super, weight_super)

table1 = data.frame(Scale = scales, Subordinate = sub, Superordinate = super)
table1 <- xtable(table1)
print(table1, type = "latex", include.rownames = FALSE, tabular.environment = "tabularx", width = "\\textwidth", floating.environment = "table*")
```

\begin{table*}
\centering
\begin{tabular}{lll}
  \hline
Scale \{\emph{adjectives}\}& Subordinate & Superordinate \\
  \hline
Height \{\emph{tall}, \emph{short}\} & professional \{basketball player, soccer player, gymnast\} & people \\
  Price \{\emph{expensive}, \emph{cheap}\}& \{dishwasher, toaster, oven\} & kitchen appliances \\
  Temperature \{\emph{warm}, \emph{cold}\}& \{Summer, Fall, Winter\} day in Maryland & days of the year \\
  Time \{\emph{long}, \emph{short}\}& \{movie, music video, video of a cute animal\}  & things you watch online \\
  Weight \{\emph{heavy}, \emph{light}\}& \{watermelon, grape, apple\}  & produce \\
   \hline
\end{tabular}
\caption{Items used in experiments}
\label{tab:1}
\end{table*}



```{r load_model_results, cache = T}
#  n_samples = 50000
#  m.samp <- data.frame()
# # 
#  chains <- c(1, 3)
#  file.prefix <- "fbt-L1-explAltnoSilence-widerPriorPriors-empiricalCC-noLength-disc3-mcmc50000_burn25000_chain"
# chains <- c(1,2,3)
#file.prefix <-"fbt-L1-explAltwSilence-widerPriorPriors-empiricalCC-noLength-disc3-mcmc50000_burn25000_chain"
# for (i in chains){
#   m <- as.data.frame(fread(paste(project.path,"models/sherlock/results/",
#                                  file.prefix, i, ".csv", sep="")))
#   m.samp.i <- rwebppl::get_samples(m, n_samples)
#   m.samp <- bind_rows(m.samp, m.samp.i)
# }

# save(m.samp, file =
#        paste(
#          project.path,
#          "writing/cogsci17/model_results/fbt-L1-explAlt-wSilence-empiricalCC-disc3-mcmc50000_burn25000_3chain.RData",
#          sep = ""))

load(paste(
   project.path,
   "writing/cogsci17/model_results/fbt-L1-explAlt-noSilence-empiricalCC-disc3-mcmc50000_burn25000_2chain.RData",
   #"writing/cogsci17/model_results/fbt-L1-explAlt-wSilence-empiricalCC-disc3-mcmc50000_burn25000_3chain.RData",
   sep = ""))

m.samp.tidy <-left_join(
  m.samp %>% filter(param == "prior"),
  d %>% select(degree, sub_category, strength) %>% unique() %>%
  rename(cat = sub_category)
) %>% 
  mutate(strength = factor(strength, levels = c(1,2,3),
                           labels = c("low","medium","high")))



m.freq.summary <- m.samp %>% filter(param == "frequency") %>%
  group_by(param) %>%
  summarize( MAP = estimate_mode(val),
             cred_upper = HPDhi(val),
             cred_lower = HPDlo(val) )

m.pp <- m.samp %>% 
  filter(param %in% c("superCC", "superSpeaker")) %>%
  group_by(param, cat,form) %>%
  summarize( MAP = estimate_mode(val),
             cred_upper = HPDhi(val),
             cred_lower = HPDlo(val) )
```


```{r scrapedWebGrams}
sub_categories <- read.csv(paste(project.path, "analysis/webgram_subcat.csv", sep = ""))
super_categories <- read.csv(paste(project.path, "analysis/webgram_supercat.csv", sep = ""))

df.freq <- bind_rows(
  sub_categories %>% 
    select(X, web.gram.freq),
  super_categories %>% 
    select(X, web.gram.freq)
) %>% drop_na() %>% spread(X, web.gram.freq)

df.freq.long <- data.frame(
  cat = c(
    "grape","apple","watermelon", "produce",
    "gymnast","soccer player","basketball player","people",
    "bottle opener", "toaster", "dishwasher", "kitchen appliances",
    "day in Winter", "day in Fall", "day in Summer", "days of the year",
    "video of the cute animal", "music video", "movie", "things you watch online"
  ),
  category = c(
    "low", "medium","high","super",
    "low", "medium","high","super",
    "low", "medium","high","super",
    "low", "medium","high","super",
    "low", "medium","high","super"
  ),
  degree = c(
    "weight","weight","weight","weight",
    "height","height","height","height",
    "price","price","price","price",
    "temperature","temperature","temperature","temperature",
    "time","time","time","time"
  ),
  freq = c(
    df.freq$grapes, df.freq$apples, df.freq$watermelons, df.freq$`fruits and vegetables`,
    df.freq$gymnasts, df.freq$`soccer players`, df.freq$`basketball players`, df.freq$people,
    df.freq$`bottle openers`, df.freq$toasters, df.freq$`dishwashing machines`, df.freq$`kitchen appliances`,
    (df.freq$`days in Winter` + df.freq$`days of Winter` + df.freq$`Winter days`)/3, 
    (df.freq$`days in Fall` + df.freq$`days of Fall` + df.freq$`Fall days`)/3,
    (df.freq$`days in Summer` + df.freq$`days of Summer` + df.freq$`Summer days`)/3, 
    (df.freq$`days in the year` + df.freq$`days of the year`)/2,
    (df.freq$`videos of animals` + df.freq$`animal videos`)/2,
    df.freq$`music videos`, df.freq$movies, 
    (df.freq$`online videos` + df.freq$`online media`)/2
  )
)

df.scaledFreq <- left_join(
  df.freq.long %>%
    filter(category != "super"),
  df.freq.long %>% 
    filter(category == "super") %>%
    select(-category, -cat) %>% 
    rename(superFreq = freq)) %>% 
  gather(key, val, freq, superFreq) %>%
  mutate(logval = log(val),
         scaledLogval = m.freq.summary$MAP *logval,
         scaledLogval_credLo = m.freq.summary$cred_lower *logval,
         scaledLogval_credHi = m.freq.summary$cred_upper *logval,
         scaledVal = exp(scaledLogval),
         scaledVal_upper = exp(scaledLogval_credHi),
         scaledVal_lower = exp(scaledLogval_credLo)) %>%
  select(-logval, -scaledLogval, -val, -scaledLogval_credLo, -scaledLogval_credHi)

df.scaledFreq.wide <- left_join(
  df.scaledFreq %>% 
    filter(key == "freq") %>% select(-key),
  df.scaledFreq %>%
    filter(key == "superFreq") %>% select(-key, -cat, category) %>%
    rename(super_val = scaledVal,
           super_lower = scaledVal_lower,
           super_upper = scaledVal_upper)
  ) %>%
  mutate(
    sumScaledFreq = scaledVal + super_val,
    sumScaledLower = scaledVal_lower + super_lower,
    sumScaledUpper = scaledVal_upper + super_upper,
    cPrior = super_val / sumScaledFreq,
    c_lower = super_lower / sumScaledLower,
    c_upper = super_upper / sumScaledUpper
    )
```


```{r expt1results, fig.env = "figure*", fig.pos = "htb", fig.width=6.6, fig.height=4.9, fig.align='center', set.cap.width=T, num.cols.cap=2, fig.cap = "Empirical data, model inferred world priors, and empirically measured comparison class priors. Top: Experiment 1 results. Comparison class judgments in terms of proportion judgments in favor of superordinate comparison class. Error bars correspond to 95\\% Bayesian credible intervals. Middle: Inferred prior distributions of world knowledge used in Experiments 1 and 2. Bottom: Inferred prior probability of the superordinate comparison class based on Google WebGram frequencies. Error bars are derived from the 95\\% credible interval on the $\\beta$ scale parameter."}

fig2a <- df.bayes %>%
  ungroup() %>%
  mutate(experiment = 'Comparison class \n data',
          sub_category = factor(sub_category, 
                  levels = c("gymnast", "soccer player", "basketball player",
                             "bottle opener", "toaster", "dishwasher",
                             "day in Winter", "day in Fall", "day in Summer",
                             "video of the cute animal", "music video", "movie",
                             "grape", "apple", "watermelon"),
                  labels = c("gymnast", "soccer player", "basketball player",
                             "bottle opener", "toaster", "dishwasher",
                             "day in Winter", "day in Fall", "day in Summer",
                             "cute animal video", "music video", "movie",
                             "grape", "apple", "watermelon") )) %>%
#                  levels = sub_category[order(strength)])) %>%
  ggplot(.,
         aes( x = sub_category, y = MAP_h, 
              ymin = low, ymax = high, 
              group = form, fill = form ) )+
  geom_bar(stat = 'identity', position = position_dodge(), alpha = 1, 
           color = 'black', width = 0.9) +
  geom_errorbar(position = position_dodge(0.9), width = 0.3) +
  facet_grid(experiment~degree, scales = 'free') +
  ylim(0, 1) +
  scale_fill_brewer(palette = 'Set3') +
  ylab("Superordinate paraphrase   ") +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.95),
        axis.title.x = element_blank(),
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        axis.line = element_line(colour = "black"))

fig2b <- ggplot(m.samp.tidy %>% mutate(experiment = "Degree \n priors"), 
                aes(x = val, y = ..scaled.., 
                 fill = strength, lty = strength))+
  geom_density(adjust = 2, size = 0.8, alpha = 0.8)+
  scale_fill_manual(values = c("#ffeda0","#feb24c","#f03b20"))+
  #scale_color_manual(values = c("#edf8b1","#7fcdbb","#2c7fb8"))+
  facet_grid(experiment~degree, scales = 'fixed')+
  xlim(-3, 3)+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 1))+
  xlab("Degree value")+
  ylab("Prior density")+
  theme(strip.text.x = element_blank(),
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        axis.line = element_line(colour = "black"))

fig2c <- ggplot(
  df.scaledFreq.wide %>% 
  mutate(
    experiment = "Empirical class prior",
   cat = factor(cat, 
  levels = c("gymnast", "soccer player", "basketball player",
             "bottle opener", "toaster", "dishwasher",
             "day in Winter", "day in Fall", "day in Summer",
             "video of the cute animal", "music video", "movie",
             "grape", "apple", "watermelon"),
  labels = c("gymnast", "soccer player", "basketball player",
             "bottle opener", "toaster", "dishwasher",
             "day in Winter", "day in Fall", "day in Summer",
             "cute animal video", "music video", "movie",
             "grape", "apple", "watermelon") ),
  degree = factor(degree, levels = c("height", "price", "temperature",
                                     "time", "weight"),
                  labels = c("people", "kitchen appliances",
                             "days of the year", "online videos", "produce")),
  strength = factor(category, levels = c("low", "medium", "high")),
  experiment  = "Comparison class \n priors"), 
  aes( x = cat, y = cPrior, fill = strength, 
       ymin = c_lower, ymax = c_upper))+
  geom_bar(stat = 'identity', position = position_dodge(), color = 'black')+
  geom_errorbar(position = position_dodge(), width = 0.5)+ 
  scale_fill_manual(values = c("#ffeda0","#feb24c","#f03b20"))+
  facet_grid(experiment~degree, scales = 'free')+
    theme(axis.text.x = element_text(angle = 90, hjust = 0.95),
        axis.title.x = element_blank(),
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        axis.line = element_line(colour = "black")) +
  ylim(0, 1) + 
  ylab("Superordinate probability")

grid.arrange(fig2a, fig2b,fig2c, nrow = 3, heights = c(2,1,2))

# bind_rows(
#   df.summary %>%
#     ungroup() %>%
#     mutate(experiment = 'Comparison class inference'),
#   vs.summary %>%
#     ungroup() %>%
#     mutate(experiment = 'Adjective production')
# ) %>%
#   mutate(sub_category = 
#            factor(sub_category, 
#                   levels = sub_category[order(strength)]),
#          experiment = factor(experiment, levels = c('Comparison class inference',
#                                                     'Adjective production'))
#           # condition = factor(condition, levels = c("context","contextWithSuper"),
#           #                    labels=c("Bare", "Supercat mention"))
#          ) %>%
#   ggplot(.,
#          aes( x = sub_category, y = mean, 
#               ymin = ci_lower, ymax = ci_upper, 
#               group = form, fill = form ) )+
#   geom_bar(stat = 'identity', position = position_dodge(), alpha = 1, 
#            color = 'black', width = 0.9) +
#   geom_errorbar(position = position_dodge(0.9), width = 0.3) +
#   #facet_grid(condition~degree, scales = 'free') +
#   facet_grid(experiment~degree, scales = 'free') +
#   ylim(0, 1) +
#   #theme(axis.text.x = element_text(angle = 90, hjust = 0.95) ) +
#   scale_fill_brewer(palette = 'Set3') +
#   ylab("Proportion superordinate paraphrase") +
#   xlab("Item category") +
#   theme(axis.text.x = element_text(angle = 90, hjust = 0.95))
#   # theme(axis.title.y = element_text(size = 10),
#   #       strip.text = element_text(size = 8))



# df.prior.combined <-bind_rows(
#   df.summary %>%
#    ungroup() %>%
#    rename(val = mean, cat = sub_category) %>% 
#   select(-super_category, -target) %>%
#   mutate(strength = as.numeric(as.character(strength)),
#          data_source = "expt1"),
#   m.samp.tidy %>% 
#     select(-param) %>% 
#     mutate(ci_lower = 0, ci_upper = 0,
#            strength = as.numeric(as.character(strength)),
#            data_source = "priors")
# )

  # ggplot(df.prior.combined, aes())+
  # facet_grid(data_source~degree) + 
  # geom_density(data = subset(df.prior.combined, data_source == "priors"), 
  #              aes (x = val, y = ..scaled.., fill = factor(strength), lty = factor(strength)))+
  # geom_bar(data = subset(df.prior.combined, data_source == "expt1"),
  #          aes(x = cat, y = mean, 
  #             group = form, fill = form ), stat = 'identity', 
  #          position = position_dodge(), alpha = 1, 
  #          color = 'black', width = 0.9) +
  # geom_errorbar(data = subset(df.prior.combined, data_source == "expt1"), 
  #               aes(ymin = ci_lower, ymax = ci_upper),
  #               position = position_dodge(0.9), width = 0.3)


```






# Experiment 1: Comparison class inference

In this experiment, we test the two predictions of our language model (outlined above).
The model and methods were preregistered: \url{https://osf.io/hzdtj/}.

## Methods

### Participants

<!--
Note: We are collecting N = 150 for each experiment. Because each participant reads have of the prompts, we expect about n = 75 per item. Simulations reveal that with n = 75, a 95% CI will have maximum width of about 0.2 - 0.25. Note that maximum width occurs when true probability is 0.5.
-->

We recruited 264 participants from Amazon Mechanical Turk.
2 participants were excluded for failing an attention check.
Participation was restricted to those with U.S. IP addresses with at least a 95% work approval rating.
The experiment took about 5 minutes and participants were compensated $0.50 for their work.


### Materials

We tested our hypothesis using positive- and negative-form gradable adjectives describing five scales (Table \ref{tab:1}).
Each scale was paired with a superordinate category, and for each superordinate category, we used three subordinate categories that aimed to be situated near the high-end, low-end, and intermediate part of the degree scale (as in Model Predictions section).
This resulted in 30 unique events (\{3 subordinate categories\} x \{5 scales\} x \{2 adjective forms\}).

### Procedure

On each trial, participants were given a context sentence to introduce the subordinate category (e.g., *Tanya lives in Maryland and steps outside in Winter.*).
This was followed by an adjective sentence, which predicated either a positive- or negative-form gradable adjective over the item (e.g., *Tanya says to her friend, "It's warm."*).
Participants were asked "What do you think Tanya meant?" and given a two-alternative forced-choice to rephrase the adjective sentence with an explicit comparison class of either the subordinate or superordinate category: 

\begin{itemize}
\item \{She / He / It\} is \textsc{adjective} (e.g., warm) relative to other \textsc{subordinates} (e.g., \emph{days in Winter}) or \textsc{superordinates} (e.g., \emph{days of the year})
\end{itemize}

<!--\item \{She / He / It\} is \textsc{adjective} relative to other \textsc{superordinates} (e.g., \emph{It's warm relative to other days of the year})-->
Each participant saw 15 trials: one for each subordinate category paired with either the positive or negative form of its corresponding adjective.
Participants never judged the same subordinate category for both adjective forms (e.g., cold and warm Winter days) and back-to-back trials involved different scales to avoid fatigue.
In addition to all of the above design parameters, half of our participants completed trials where an additional sentence introduced the superordinate category at the beginning (e.g., *Tanya lives in Maryland and checks the weather every day.*), with the intention of making the superordinate paraphrase more likely.

The experiment can be viewed at:
\url{http://stanford.edu/~mtessler/comparison-class/experiments/class-elicitation-2afc.html}

## Results

```{r mixed_model, cache = T}
d.centered = d.tidy %>%
                   filter(!(strength == 2)) %>%
                   mutate(strength = ifelse(strength == 3, 1, 
                                            ifelse(strength == 1, 0,
                                                   -99)),
                          c.strength = strength-mean(strength),
                          num.form = as.numeric(form) - 1,
                          c.form = num.form - mean(num.form))

# with interaction random effects
rs.glmm <- glmer(data = d.centered, 
                 superResponse ~ c.form*c.strength +
                   (1 + c.form:c.strength | sub_category) +
                   (1 | workerid),
                 family = 'binomial')
rs.glmm.summary <-  summary(rs.glmm)
 
#simple effect model doesn't converge w/ interaction random effects
d.centered$strength = as.factor(d.centered$strength)
rs.glmm.simple <- glmer(data = d.centered, 
                 superResponse ~ strength*form - form +
                   (1  | sub_category) +
                   (1  | workerid),                   
                 family = 'binomial')
rs.glmm.simple.summary <- summary(rs.glmm.simple)



```

We observe no systematic differences between participants' responses when the superordinate category was explicitly mentioned previously in the context and those when it was not; thus we collapse across these two conditions for all subsequent analyses.
Figure \ref{fig:expt1results} (top) shows the proportion of participants choosing the superordinate paraphrase for each item.
We see considerable variability both within- and across- scales.
Within each scale, the predicted interaction is visually apparent: For items that are expected to fall high on the scale (basketball players, watermelons, Summer days, dishwashers, movies), positive form adjectives tend to elicit superordinate comparison classes while negative form adjectives tend to elicit subordinate comparison classes (e.g., a basketball player is *tall* relative to other people, but *short* relative to other basketball players).
For items that are expected to fall low on the scale (gymnasts, grapes, Winter days, bottle openers, videos of cute animals), the opposite effect is observed (i.e., a video of a cute animal is *short* relative to other things you watch online, but *long* relative to other videos of cute animals).

This is confirmed using a generalized linear mixed effects model with main effects of adjective form (positive vs. negative) and the *a priori* judgment by the first author of the relative position on the scale of the sub-category (i.e., sub-category item expected to be low or high), and of critical theoretical interest, the interaction between these two variables
In addition, we included in the model by-participant random effects of intercept and by-subordinate category random effects of intercept and iteraction between form and strength to predict the probability with which participants would choose the superordinate category as the paraphrase^[
This was the maximal mixed-effects structure that converged.
].
Confirming our two qualitative model predictions, there was an overall preference for subordinate category paraphrases ($\beta = `r round(rs.glmm.summary[["coefficients"]]["(Intercept)","Estimate"],2)`$;
$SE = `r round(rs.glmm.summary[["coefficients"]]["(Intercept)","Std. Error"],2)`;$
$z = `r round(rs.glmm.summary[["coefficients"]]["(Intercept)","z value"],2)`$) and there was an interaction between form and strength 
($\beta = `r round(rs.glmm.summary[["coefficients"]]["c.form:c.strength","Estimate"],2)`$;
$SE = `r round(rs.glmm.summary[["coefficients"]]["c.form:c.strength","Std. Error"],2)`;$
$z = `r round(rs.glmm.summary[["coefficients"]]["c.form:c.strength","z value"],2)`$).
The main effects of form and strength were not significant.

Following up on this model, we examine the simple effects of strength for both positive and negative form adjectives.
We find the predicted effects on superordinate paraphrase probability for items both on the low and high end of the scale. 
For items on the low end of the scale (e.g., Winter days on temperature), positive form adjectives are significantly more likely to lead *away* from superordinate comparison classes (
$\beta = `r round(rs.glmm.simple.summary[["coefficients"]]["strength0:formpositive","Estimate"],2)`$;
$SE = `r round(rs.glmm.simple.summary[["coefficients"]]["strength0:formpositive","Std. Error"],2)`;$
$z = `r round(rs.glmm.simple.summary[["coefficients"]]["strength0:formpositive","z value"],2)`$), while the opposite is true for items on the high end of the scale (e.g., Summer days; 
$\beta = `r round(rs.glmm.simple.summary[["coefficients"]]["strength1:formpositive","Estimate"],2)`$;
$SE = `r round(rs.glmm.simple.summary[["coefficients"]]["strength1:formpositive","Std. Error"],2)`;$
$z = `r round(rs.glmm.simple.summary[["coefficients"]]["strength1:formpositive","z value"],2)`$).
All of these effects are also visually apparent in Figure \ref{fig:expt1results} (top).

\ndg{this section can be tightened up a bunch?}

# Experiment 2: Adjective endorsement



<!-- Begin LaTeX code for graphical model -->
<!-- \usetikzlibrary{bayesnet} -->
<!-- \begin{figure}[ht] -->
<!-- \captionsetup{width=0.48\textwidth} -->
<!-- \begin{center} -->
<!-- \resizebox{6.5cm}{6.3cm}{ -->
<!-- \begin{tikzpicture}[ -->
<!-- rsa/.style={circle, draw=red!60, fill=red!5, very thick, minimum size=7mm}, -->
<!-- discrete_obs/.style={rectangle,fill=gray!25,draw=black,inner sep=1pt, minimum size=20pt, font=\fontsize{10}{10}\selectfont, node distance=1}, -->
<!-- ] -->


<!-- \node[rsa, minimum size=1cm] (L0_speaker) {$L_{0}$} ; -->
<!-- \node[rsa, minimum size=1cm, below=of L0_speaker] (S1_speaker) {$S_{1}$} ; -->
<!-- \node[rsa, minimum size=1cm, below=of S1_speaker] (L1_speaker) {$L_{1}$} ; -->
<!-- \node[rsa, minimum size=1cm, below=of L1_speaker] (S2_speaker) {$S_{2}$} ; -->

<!-- \node[latent, minimum size=1cm, left=of S1_speaker, xshift=-2cm] (alpha_1) {$\alpha_1$} ; -->
<!-- \node[discrete_obs, minimum size=1cm, right=of L1_speaker, xshift=2cm] (d_c) {$d_{c}$} ; -->
<!-- \node[latent, minimum size=1cm, left=of S2_speaker, xshift=-2cm] (alpha_2) {$\alpha_2$} ; -->
<!-- \node[discrete_obs, minimum size=1cm, right=of S2_speaker, xshift=2cm] (d_u) {$d_{u}$} ;   -->

<!-- \node[latent, minimum size=1cm, right=of L0_speaker] (comparison_class_posterior) {$x | c_{sub}$} ; -->

<!-- \node[latent, minimum size=1cm, right=of comparison_class_posterior, yshift=0.6cm] (mu) {$\mu_{sub}$} ; -->
<!-- \node[latent, minimum size=1cm, right=of comparison_class_posterior, yshift=-0.6cm] (sigma) {$\sigma_{sub}$} ; -->

<!-- \node[latent, minimum size=1cm, left=of L1_speaker] (comparison_class) {$c$} ; -->
<!-- \node[latent, minimum size=1cm, left=of comparison_class, yshift=0.6cm] (ngrams) {$\hat{f}$} ; -->
<!-- \node[latent, minimum size=1cm, left=of comparison_class, yshift=-0.6cm] (beta) {$\beta$} ; -->

<!-- \edge {alpha_1} {S1_speaker} ; -->
<!-- \edge {L1_speaker} {d_c} ; -->
<!-- \edge {alpha_2} {S2_speaker} ; -->
<!-- \edge {S2_speaker} {d_u} ; -->

<!-- \edge {comparison_class_posterior} {L0_speaker} ; -->
<!-- \edge {comparison_class_posterior} {L1_speaker} ; -->

<!-- \edge {mu} {comparison_class_posterior} ; -->
<!-- \edge {sigma} {comparison_class_posterior} ; -->

<!-- \edge {comparison_class} {L1_speaker} ; -->
<!-- \edge {ngrams} {comparison_class} ; -->
<!-- \edge {beta} {comparison_class} ; -->

<!-- \plate {} { -->
<!-- 	(comparison_class_posterior)(L0_speaker)(S1_speaker)(L1_speaker)(S2_speaker)(comparison_class) -->
<!-- } {RSA} -->

<!-- \plate {} { -->
<!-- 	(mu)(sigma)(comparison_class_posterior) -->
<!-- } {$\forall sub \in \mathcal{S}$} -->

<!-- \end{tikzpicture} -->
<!-- } -->
<!-- \end{center} -->
<!-- \caption{A representation of the full RSA - data analysis model. RSA cannot actually be visualizing using standard graphical model notation; each of its component parts are functions that are defined in terms of each other. $L_1$ is used as a model for Experiment 1 data and  $S_2$ for Experiment 2 data. The subordinate category priors $P(x \mid c_{sub})$ are used in both experiments and are modeled as Gaussians with unknown mean and variance. The comparison class prior $P(c)$ is modeled by an empirical measurement of frequency $\hat{f}$ and a free parameter $\beta$. (See main text for priors.)} -->
<!-- \label{fig:modelDiagram} -->
<!-- \end{figure} -->
<!-- End LaTeX code for graphical model -->

<!-- \ndg{i'm afraid the 'graphical model' will have to go...} -->

In this experiment, we use a two-alternative forced choice paradigm to collect participants' adjective endorsements using knowledge that would be relevant for Expt. 1.

## Participants

We recruited 100 participants from Amazon Mechanical Turk.
5 participants were excluded due to failing the catch trial.
Participation was restricted to those with U.S. IP addresses and who had at least a 95% work approval rating.
On average, the experiment took 5 minutes and participants were compensated $0.50 for their work. 

## Materials and procedure

Materials were the same as Expt. 1 (Table \ref{tab:1}).
On each trial, participants were given a sentence introducing the subordinate category (e.g., *Alicia lives in Maryland and steps outside in Winter.*).
This was followed by a target sentence, which predicated a positive- or negative-form adjective over the superordinate category of the item (e.g., *Do you think the day in Winter would be warm relative to other days of the year?*).
Each participant saw 15 trials: one for each subordinate category paired with either the positive or negative form of its corresponding adjective.
As in Expt. 1, participants never rated the same subordinate category for both adjective forms and back-to-back trials involved different scales to avoid fatigue.
The experiment can be viewed at: 
\url{http://stanford.edu/~mtessler/comparison-class/experiments/vague-prior-elicitation-2afc.html}

## Results

The results of this experiment were as expected and can be seen roughly in Figure \ref{fig:posteriorPredictive_scatters} (right, y-axis). 
We see that the endorsement of adjectival phrases in these domains is markedly more categorical than the comparison class inference task.


# Bayesian data analysis

The full RSA model with uncertainty over the priors has a number of parameters. 
The comparison class prior uses scaling parameter on the empirical frequency $\hat{f}$, which we give the following prior: $\beta \sim \text{Uniform}(0, 3)$. 
Each subordinate category prior uses standardized units (given by the superordinate Unit-Normal), and put the same priors over the parameters of each subordinate Gaussian: $\mu \sim \text{Uniform}(-3, 3)$, $\sigma \sim \text{Uniform}(0, 5)$.

The full model has three additional parameters not of direct theoretical interest: the speaker optimality parameters $\alpha^\text{expt}_{i}$, which can vary across the two tasks (see Figure \ref{fig:modelDiagram} for visual overview of the full model).
Expt. 1 uses the pragmatic listener $L_1$ model, which has one speaker optimality: $\alpha^\text{1}_{1}$.
Expt. 2 uses the pragmatic speaker $S_2$ model, which has two speaker optimality parameters: 
$\{\alpha^\text{2}_{1}, \alpha^\text{2}_{2}\}$
We use the priors consistent with models of this class: $\alpha_1 \sim \text{Uniform}(0, 20)$, $\alpha_2 \sim \text{Uniform}(0, 5)$

## Results

We implemented the RSA and Bayesian data analysis models in the probabilistic programming language WebPPL [@dippl].
To learn about the credible values of the parameters and the predictions of the model, we used an incrementalized version of MCMC [@Ritchie2016], collecting 2 independent chains of 50,000 iterations (removing the first 25,000 for burn-in).

<!--

-->
```{r}

md.pp <- left_join(
  bind_rows(vs.bayes %>% 
             mutate(expt = "superSpeaker"),
           df.bayes %>%
             mutate(expt = "superCC")) %>% 
             ungroup() %>%
             rename(cat = sub_category),
  m.pp %>% rename(expt = param)) %>%
  mutate(expt = factor(expt, 
                       levels = c("superSpeaker", "superCC"),
                       labels = c('Adjective production', 'Comparison class inference')),
         form = factor(form, levels = c("positive", "negative")))

         
fill.colors <- RColorBrewer::brewer.pal(5, "Set3")
names(fill.colors) <- levels(md.pp$degree)

scatterFig <- ggplot(md.pp, aes(x = MAP, xmin = cred_lower, xmax = cred_upper,
                  y = MAP_h, ymin = low, ymax = high,
                  shape = form, fill = degree))+
  geom_abline(intercept = 0, slope = 1, lty = 3)+
  geom_errorbar(size = 0.5, alpha = 0.4)+
  geom_errorbarh(size = 0.5, alpha =0.4)+
  geom_point(size = 2.5, color = 'black')+
  scale_shape_manual(values = c(24, 25))+
  scale_fill_brewer(palette = "Set3")+
  facet_wrap(~expt)+
  coord_fixed(ratio = 1)+
  xlim(0, 1)+
  ylim(0, 1)+
  ylab("Human endorsement")+
  xlab("Model prediction")+
  theme(legend.box = "horizontal",
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        axis.line = element_line(colour = "black"),
        #legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_blank(),
        legend.position = c(0.57, -0.2))+
  guides(fill=guide_legend(override.aes=list(colour=fill.colors)), shape = F)
        

expt1.r2 <- round(with(md.pp %>%
       filter(expt == "Comparison class inference"), cor(MAP, MAP_h))^2, 3)

expt1.n <- length(filter(md.pp, expt == "Comparison class inference")[["MAP"]])

expt2.r2 <- round(with(md.pp %>%
       filter(expt == "Adjective production"), cor(MAP, MAP_h))^2, 3)

expt2.n <- length(filter(md.pp, expt == "Adjective production")[["MAP"]])
```


```{r posteriorPredictive_scatters, fig.env = "figure*", fig.pos = "htb", fig.width=6.6, fig.height=4, fig.align='center', set.cap.width=T, num.cols.cap=2, fig.cap = "Human endorsement of comparison class paraphrases (middle) and adjective sentences (left) as a function of listener model $L_{1}$ and speaker model $S_{2}$ predictions, respectively. The right facet displays a subset of the paraphrase data (Expt. 1) to reveal good quantitative fit even in a small dynamic range. Error bars correspond to 95\\% Bayesian credible intervals for the participant data and model predictions."}
scatterSubFig <- ggplot(md.pp %>% filter(MAP_h < 0.30 & expt == "Comparison class inference") %>%
                          mutate(expt = factor(expt, 
                                               labels = c("Comparison class inference (subset)"))), 
       aes(x = MAP, xmin = cred_lower, xmax = cred_upper,
                  y = MAP_h, ymin = low, ymax = high,
                  shape = form, fill = degree))+
  geom_abline(intercept = 0, slope = 1, lty = 3)+
  geom_errorbar(size = 0.5, alpha = 0.2)+
  geom_errorbarh(size = 0.5, alpha = 0.2)+
  geom_point(size = 2.5, color = 'black')+
  scale_shape_manual(values = c(24, 25))+
  scale_fill_brewer(palette = "Set3")+
  facet_wrap(~expt)+
  coord_fixed(ratio = 1)+
  xlim(0, 0.3)+
  ylim(0, 0.3)+
  ylab("Human endorsement")+
  xlab("Model prediction")+
  theme(#legend.position = "bottom",
        legend.position = c(0.1,-0.2),
        legend.direction = "horizontal",
        legend.title = element_blank(),
       # panel.grid.minor = element_blank(),
      #  panel.grid.major = element_blank(),
        axis.line = element_line(colour = "black"))+
  guides(fill = F)
  
grid.arrange(scatterFig, scatterSubFig, nrow = 1, widths = c(2, 1.07))
```



```{r parameterPosteriors}
m.so <- m.samp %>% filter(cat %in% c("speakerOptimality_s1", "speakerOptimality_s2")) %>%
  separate(cat, into = c("cat", "speaker"))  %>%
  group_by(cat, param, speaker) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = HPDhi(val),
            cred_lower = HPDlo(val))

m.freq <- m.samp %>% filter(cat %in% c("silenceCost","explicitCost","superCatPrior", "beta")) %>%
  group_by(cat) %>%
  summarize(MAP = estimate_mode(val),
            cred_upper = HPDhi(val),
            cred_lower = HPDlo(val))

a1.1 <- filter(m.so, param == "ccRSA" & speaker == "s1")
a1.2 <- filter(m.so, param == "superSpeaker" & speaker == "s1")
a2.2 <- filter(m.so, param == "superSpeaker" & speaker == "s2")

a1.1.map <- round(a1.1[["MAP"]],2)
a1.1.lower <- round(a1.1[["cred_lower"]],2)
a1.1.upper <- round(a1.1[["cred_upper"]],2)

b.map <- round(m.freq[["MAP"]],2)
b.lower <- round(m.freq[["cred_lower"]],2)
b.upper <-  round(m.freq[["cred_upper"]],2)

a1.2.map <- round(a1.2[["MAP"]],2)
a1.2.lower <- round(a1.2[["cred_lower"]],2)
a1.2.upper <- round(a1.2[["cred_upper"]],2)

a2.2.map <- round(a2.2[["MAP"]],2)
a2.2.lower <- round(a2.2[["cred_lower"]],2)
a2.2.upper <- round(a2.2[["cred_upper"]],2)
```

The full model's posterior predictive distribution does an excellent job at capturing the variability in responses for Expt. 1: $r^2(`r expt1.n`) = `r expt1.r2`$, and Expt. 2: $r^2(`r expt2.n`) = `r expt2.r2`$ (Figure \ref{fig:posteriorPredictive_scatters}).
The inferred values and shapes of the world knowledge used in these tasks is also consistent with intuition, as can be see in Figure \ref{fig:expt1results} (middle).
The items judged *a priori* to be low the on scale (yellow) tend to be lower than those judged to be in the middle of the scale (orange) and high on the scale (red).

\ndg{i don't think we need this:
The most residual uncertainty is in the heights of "soccer players" and the duration of "cute animal videos". 
The former may be due to a discrepancy between participants' judgments in the comparison class task, where participants tended to say soccer players were either tall or short "relative to other people", suggesting the heights of soccer players and the heights of people in general are similar; in the adjective speaker task, there was a small tendency for participants to say soccer players would be tall (but not short) relative to other people. 
The uncertainty remanining in the duration of "cute animal videos" may be a result of our empirical corpus frequency $\hat{f}$ for the inferred comparison class prior (Figure \ref{fig:expt1results} bottom): "Cute animal videos" and "online videos", the relevant subordinate and superordinate categoires, were among the most *infrequent* ngrams in the corpus, and thus the relative measurement probably has more noise than other items. 
}

The maximum a-posteriori (MAP) estimate and 95\% highest probability density (HPD) interval for model parameters specific to the $L_1$ model used for Experiment 1 were $\alpha^{1}_{1} = `r a1.1.map` [`r a1.1.lower`, `r a1.1.upper`]$; $\beta = `r b.map` [`r b.lower`, `r b.upper`]$ 
Model parameters specific to the $S_2$ model used for Experiment 2: $\alpha^{2}_{1} = `r a1.2.map` [`r a1.2.lower`, `r a1.2.upper`]$; $\alpha^{2}_{2} = `r a2.2.map` [`r a2.2.lower`, `r a2.2.upper`]$.

# Discussion

Vagueness in language (e.g., saying "It's warm outside") poses a challenge to listeners, who have to use context to uncover a more precise meaning to make sense of what they've heard.
Context, however, can also be underspecified, as there are many possible dimensions or categories that the speaker might be implicitly referring to or comparing against.
Here, we investigate the flexibility in the reference class against which an entity can be implicitly compared.
We introduce a minimal extension of an adjective interpretation RSA model in order to flexibly reason about the likely comparison class.
This model made two novel predictions: 
(1) Listeners should prefer comparison classes that have relatively lower variance as they result in more specific meanings and 
(2) The comparison class will shift with the *a priori* probability of the adjective being true of the lower variance (subordinate) class.
When the target entity is from a subordinate class with a mean towards the upper part of the scale (e.g., temperature during Summer), a positive form adjective (e.g., warm) should lead listeners to infer a more general comparison class than a negative form adjective (e.g., cold). 
The opposite is predicted to be true of entities towards the lower end of the scale (e.g., days in Winter).
\ndg{don't need to reiterate the predictions in such agonizing detail. maybe just say there were two qualitative predictions and quantitative predictions and they all worked out?}
Both of these predictions were borne out in our first experiment.
To our knowledge, this is the first experiment to demonstrate how reference classes for adjective interpretation can adjust based on world knowledge.

The second contribution of this paper is a novel data-analytic approach, where prior knowledge used in the Bayesian language model is reconstructed from converging evidence gathered from experiments that use similar language about the same domains.
In previous work, we have attempted to measure prior knowledge by decomposing what would be implicitly multilayered questions into multiple simpler questions, and then using a Bayesian data analytic model to reconstruct the prior knowledge [@Tessler2016; @Tessler2016cogsci].
We extend this approach to ask related *natural language* questions across two experiments to be able to infer the parameters of the relevant prior knowledge.
The major feature of this approach is that participants respond only to simple natural language questions rather than estimating numerical quantities for which complicated linking functions must be designed [@FrankeEtAl2016].
It also provides a further test of the language model, which must predict data from two similar but distinct language experiments. 
Modeling language use, which is itself productive, yields productivity in experiment design, which can be harnessed to reduce uncertainty in the necessary parameters of language model 

In our modeling work, we had to specify a prior distribution over the two comparison class alternatives used in Expt. 1: $P(c)$.
We posited that the adjective phrase "It's warm outside" is an incomplete sentence requiring a completion analagous to those investigated with noisy-channel models of production and comprehension [e.g., @Bergen2015].
As a proxy for production or reference probabilities, we gathered empirical priors based on n-gram frequency from the Google WebGram corpus. 
These probabilities likely conflate multiple cognitive sources such as basic level effects in categorization [@Rosch1975] and the hetereogeneity of different classes (e.g., the distribution over prices of kitchen appliances may be more hetereogenous than the distribution of heights for people).
Future work should attempt to disentangle these factors to construct a more complete theory of the comparison class prior.

<!--
One small deviation of our model from the human data is seen in the prices of kitchen appliances.
The model tends to show an intermediate preference for the superordinate comparison class, *kitchen appliances*, because the phrase *kitchen appliances* is relatively frequent in the corpus.
Participants, however, are quite reluctant to endorse this as a likely reference class.
This may be because the reference class is also sensitive to the goals of the speaker.
It's unlikely, for instance, for someone to go to a store to purchase a *kitchen appliance* (any appliance will do).
Rather, objects are purchased in order to acquire their function, and different kinds of objects have different functions.
\mht{this is rough... goal of this is to further elucidate the richness of the phenomenon.}
-->
### Conclusion 

The words we say are often too vague to have a single, precise meaning, and only make sense in context. 
The context, however, can also be underspecified, leaving the listener in the dark about both the speaker's intended meaning and about the context through which the listener is to make sense of the conversation.
Listeners, however, are able to jointly infer a lot from a little: The context and the meaning from a single utterance.
    
<!--
\begin{enumerate}
\item Speaker knowledge: If you're a basketball scout, and you say of a player that "He is tall." it means "Tall relative to basketball players"
\item QUD: If we're deciding what to do on Friday night, you say "The opera is expensive" it means "Expensive relative to other things we could do on Friday night"
\item Hyperbole / *normative* comparison classs: If we listen to a lecture, and you say "That *was long*.", it means "Long relative to how long I think it should have been." If we go out for pasta, and you ask how it was, and I say "it was expensive" it means "expensive relative to how much it should have been given the quality".
\item Item heterogeneity: Movies is not a subordinate to "things you watch online", but the model still works
\end{enumerate}
-->



<!--
Bayesian models of cognition and language strongly rely upon accurate measurement of prior knowledge [@FrankeEtAl2016].
For models of language understanding, prior elicitation tasks typically take the form of running the same language understanding task but removing the target utterance that is aimed to be model [e.g., @Kao2014]. 
Certain quantities and probabilities are inherent difficult measure because they are abstract or hard to estimate.
In previous work, we have attempted to measure prior knowledge by decomposing what would be a single, complicated question into two simpler questions, and then using a Bayesian data analytic approach to reconstruct the prior knowledge [@Tessler2016; @Tessler2016cogsci].
Certain quantities and probabilities are inherently difficult to measure because they are abstract or hard to estimate.
Here, we extend this to ask *natural language* questions that use the same prior knowledge as would be relevant for Expt.~1 and which can be interpreted by the same language model.
This has the feature of reducing task demand on participants.
Instead, we ask similar questions in the same domains to a separate set of participants, and use the data from both experiments to gain more certainty about the relevant prior knowledge.

We take a different approach. 
We assume a simple functional form to the prior and infer the likely parameter values of those priors from the data and our language model.
This by itself simply adds extra parameters to the model, and we wouldn't know if the "prior knowledge" we're learning would generalize to other tasks that require the same knowledge. 
To alleviate this problem of overfitting, we run a second experiment with the same language model as our guide. 
We ask similar questions in the same domains, and use the data from both experiments to gain more certainty about the relevant prior knowledge.

These $P(x \mid c)$ are used in both the experiments, which allows us to both simultaneously gain credibility in our estimates of these parameters as well as in our model, which has to predict data from two distinct experiments.
-->



# Acknowledgements

This work was supported in part by NSF Graduate Research Fellowship DGE-114747 (to MHT), \red{by ... (to MLB)}, \red{by ... (to NDG)}.
The authors would like to thank Ali Horowitz for help in stimuli design.

# References

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in}
\setlength{\leftskip}{0.125in}
\noindent


<!--
# Outline

1. We introduce an extension to RSA that does inference over what is the comparison class.

2. We take a simple case, where the comparison class is either a sub- or super-ordinate category. Just playing with the parameters off the model, we see this predicted (qualitative) interaction:

- When the subclass has a high mean relative to the superclass, positive form adjectives signal the superclass, and negative form signals the subclass

- When the subclass has a low mean relative to the superclass, positive form adjectives signal the subclass, and negative form signals the superclass

3. We test this predictions on 5 scales (Expt. 1)

- We see the qualitative effect on all 5 scales, but there is considerable heterogeneity among the scales.

4. This heterogeneity might be attributed to differences in the quantitative details (i.e., the parameters) of the subclass vis-a-vis the superclass

- We can perform BDA to see if this is true, but this model is actually overparameterized.

- We can simplify by assuming each superclass has a unit-normal prior, and infer the mean and standard deviation for each subclass prior.

- That’s 2 parameters for each subclass, and we only have 2 items for each subclass (namely, positive and negative form adjectives e.g., “tall” and “short” bball players)

5. We estimate the prior parameters by asking other questions of our model that should (a) access the same priors; and (b) not add other parameters

so we can ask a vague speaker question (Expt 2. [VPE])

This will alleviate the overparameterization problem and is, in general, a new way of testing language understanding models without having to explicitly measure priors
-->
