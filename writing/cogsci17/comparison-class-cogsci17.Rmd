---
title: "Cheap cars, expensive coffee: Comparison class understanding in vague language"
bibliography: [comparison-class.bib, library.bib]
csl: "apa6.csl"
document-params: "10pt, letterpaper"
header-includes:
  - \usepackage{tabularx}
  - \usepackage{multicol}
  - \usepackage{wrapfig}

author-information: >
  \author{{\large \bf Michael Henry Tessler$^1$} (mtessler@stanford.edu) \and {\large \bf Michael Lopez-Brau$^2$} (lopez\_mic@knights.ucf.edu) \\
  {\large \bf Noah D. Goodman$^1$} (ngoodman@stanford.edu) \\
  $^1$Department of Psychology, Stanford University,
  $^2$Department of Electrical \& Computer Engineering, University of Central Florida}

abstract:
    "Gradable adjectives (e.g., *expensive*) exhibit a sensitivity to context that poses a challenge to quantitative theories of language understanding.
    In particular, the issue of how to derive a comparison class---the set of entities against which a target object is implicitly compared---in context has yet to be addressed by formal models of adjective interpretation.
    We introduce a Rational Speech Act model that represents the implicit comparison class as a random variable inferred via pragmatic reasoning.
    We find this model makes the prediction that the level of abstraction of the comparison class (e.g., expensive relative to *toasters* or *ovens* vs. *kitchen appliances*) is expected to be modulated by the properties of the target object under consideration (e.g., a toaster or oven).
    We test this prediction using a paraphrase experiment.
    Quantative predictions of the model rely upon quantitative details of relevant prior distributions.
    We introduce a novel Bayesian data-analytic method for inferred participants' relevant prior knowledge by asking simple, natural language questions.
    "

  
keywords:
    "comparison class; pragmatics; Bayesian cognitive model; Bayesian data analysis"

output: cogsci2016::cogsci_paper
---

\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}
\definecolor{Orange}{RGB}{255,153,0}

\newcommand{\red}[1]{\textcolor{Red}{#1}}  
\newcommand{\ndg}[1]{\textcolor{Green}{[ndg: #1]}}  
\newcommand{\mht}[1]{\textcolor{Blue}{[mht: #1]}}  
\newcommand{\mlb}[1]{\textcolor{Orange}{[mlb: #1]}}

```{r global_options, include=FALSE}
rm(list=ls())

# set local path for the repo
#project.path <- "/Users/mht/Documents/research/comparison-class/"
project.path <- "/media/michael/Data/Desktop/comparison-class/"

knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```


```{r, libraries}
library(png)
library(grid)
library(tidyverse)
library(xtable)
library(rwebppl)
library(langcog)
library(coda)

estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}

HPDhi<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

HPDlo<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}
```


```{r}
theme_paper <- function(base_size = 12, base_family = "Helvetica") {
  theme(
    line =    element_line(colour = "black", size = 0.5, linetype = 1,
                                      lineend = "butt"),
    rect =   element_rect(fill = "black", colour = "black", size = 0.5, linetype = 1),
    text =               element_text(family = base_family, face = "plain",
                                      colour = "black", size = base_size,
                                      hjust = 0.5, vjust = 0.5, angle = 0, lineheight = 0.9,
                                      margin = margin(2,1,3,1), debug = FALSE),
    axis.text =          element_text(size = 12, colour = "black", margin=margin(1,1,1,1)),
    strip.text =         element_text(size = 12, colour = "black", margin=margin(1,1,4,1)),
    
    axis.line =          element_blank(),
    axis.ticks =         element_line(colour = "black", size = 0.2),
    axis.title =         element_text(colour = "black", size=16),
    axis.ticks.length =  unit(0.3, "lines"),

    legend.background =  element_rect(colour = "white", fill='white'),
#    legend.margin =      unit(0.2, "cm"),
    legend.key =         element_rect(fill = "white", colour = "white"),
    legend.key.size =    unit(1.2, "lines"),
    legend.key.height =  NULL,
    legend.key.width =   NULL,
    legend.text =        element_text(size = rel(1.2), colour = "black"),
    legend.text.align =  NULL,
    legend.title =       element_text(size = rel(1.2), face = "bold", hjust = 0, colour = "black"),
    legend.title.align = NULL,
    legend.position =    "right",
    legend.direction =   "vertical",
    legend.justification = "center",
    legend.box =         NULL,
    
    panel.background =   element_rect(fill = "white", colour = NA),
    panel.border =       element_rect(fill = NA, colour = "black"),
    panel.grid.major =   element_line(colour = "white", size = 0.2),
    panel.grid.minor =   element_line(colour = "white", size = 0.5),
    panel.spacing =       unit(0.5, "lines"),

    strip.background =   element_rect(fill = "white", colour = "white"),
    strip.text.x =       element_text(),
    strip.text.y =       element_text(angle = -90),
    
    plot.background =    element_rect(colour = "white", fill = "white"),
    plot.title =         element_text(size = rel(1.2)),
    plot.margin =        unit(c(1, 1, 0.5, 0.5), "lines"),
    
    complete = TRUE
  )
}
theme_set(theme_classic())
```


<!--
# Outline

1. We introduce an extension to RSA that does inference over what is the comparison class.

2. We take a simple case, where the comparison class is either a sub- or super-ordinate category. Just playing with the parameters off the model, we see this predicted (qualitative) interaction:

- When the subclass has a high mean relative to the superclass, positive form adjectives signal the superclass, and negative form signals the subclass

- When the subclass has a low mean relative to the superclass, positive form adjectives signal the subclass, and negative form signals the superclass

3. We test this predictions on 5 scales (Expt. 1)

- We see the qualitative effect on all 5 scales, but there is considerable heterogeneity among the scales.

4. This heterogeneity might be attributed to differences in the quantitative details (i.e., the parameters) of the subclass vis-a-vis the superclass

- We can perform BDA to see if this is true, but this model is actually overparameterized.

- We can simplify by assuming each superclass has a unit-normal prior, and infer the mean and standard deviation for each subclass prior.

- That’s 2 parameters for each subclass, and we only have 2 items for each subclass (namely, positive and negative form adjectives e.g., “tall” and “short” bball players)

5. We estimate the prior parameters by asking other questions of our model that should (a) access the same priors; and (b) not add other parameters

so we can ask a vague speaker question (Expt 2. [VPE])

This will alleviate the overparameterization problem and is, in general, a new way of testing language understanding models without having to explicitly measure priors
-->

If you're going to San Francisco, be sure to set aside a few extra dollars for coffee.
It's good, but expensive.
By expensive, we mean *expensive relative to coffee elsewhere*, and not *expensive relative to other things in San Francisco*: Rent in San Francisco is expensive in a different way.
This may seem obvious, and yet adjectives like *expensive* derive their relatively precise meanings from the category of things against which the target entity (e.g., the price of a good cup of coffee in San Francisco) is compared.
This *comparison class* is not often explicitly stated, bur rather is latent in the discourse.

Adjectives like *cheap* and *expensive*, *cold* and *warm* are vague descriptions of an underlying quantitative scale (e.g., price, temperature).
Contemporary linguistic theories posit that the truth-conditional semantics of such vague utterances are simple thresholds on the measure [@Kennedy2007].
For example, a good cup of coffee is expensive if its price is greater than some threshold number of dollars.
The crux of such theories rests on the fact that this threshold is relative to a *comparison class* of other possible entities.
A good cup of coffee in San Francisco is expensive *relative to* other cups of coffee and not *relative to* other things you could buy in San Francisco (including, e.g., a Tesla automobile).

@Lassiter2013 demonstrated that listener's with relevant world knowledge (e.g., prices of coffee) can use pragmatic principles to arrive at a context-dependent meaning of a gradable adjective (e.g., expensive) assuming nothing in particular about what the adjective means out of context.
They elaborate the Rational Speech Act theory [for a review, see @Goodman2016] such that the truth-conditional threshold for the adjective semantics (i.e., the point at which something becomes expensive) comes from an uninformed prior distribution: *a priori* anything could be expensive.
Listeners arrive at context-sensitive meanings by integrating their prior beliefs (e.g., knowledge of what a cup of coffee should cost) with general principles of communication: be truthful, be informative.
The result is vague language understanding that displays context-sensitivity and admits borderline cases [@Lassiter2013; @Lassiter2015].
See @Qing2014 for a related but alternative derivation of a probabilistic pragmatics model of gradable adjective interpretation.
\mht{this paragraph sucks}

In @Lassiter2013's theory of adjective interpretation, the comparison class is implicit in the listener's prior belief distribution.
But how does a listener settle on the appropriate comparison class, when none is specifically articulated by the speaker?
This question is not just an issue for gradaable adjectives but in fact for any vague language that can be modeled using a threshold semantics [e.g., generic language, see @Tessler2016]

<!--
The comparison class can depend upon the target object (as in the examples above) as well as the physical context or the Question Under Discussion.
@Ebeling1994 demonstrate that 2-4 year olds understand the context-sensitivity of the words "big" and "little," finding the appropriate object whether the object is in a physical context ("big" relative to other objects in a scene), a functional context ("big" relative to its intended use), or out of context ("big" relative to an implicit comparison class).
-->

In this paper, we argue that pragmatic principles can be used to resolve the comparison class in context.
We extend the Rational Speech Act theory of @Lassiter2013 to include uncertainty over the comparison class.
We test this model in a simplified setting, where the comparison class could either be a subordinate or superordinate class (e.g., toasters vs. kitchen appliances).
In this simplified setting, we find the model predicts the level of abstraction (sub vs. superordinate) of a comparison class depends upon the quantitative details of the relevant prior beliefs (e.g., the prices of toasters vs. the prices of kitchen appliances in general).
We test this prediction using a paraphrase experiment (Expt. 1).
Quantitative predictions for Bayesian models (including this one) depend upon the quantitative details of listeners' prior distributions over various scales.
Because it may be difficult for human participants to estimate quantities accurately, we introduce a novel Bayesian data-analytic technique for learning about listeners' prior belief distributions using convergent language understanding tasks.


# Computational model

We elaborate the Rational Speech Act model for vague language understanding [@Lassiter2013; @Lassiter2015] by introducing uncertainty over the class of entities against which the target entity is compared--- the comparison class $c$.
This uncertainty is posited at the level of the pragmatic listener, and thus is resolved using pragmatic reasoning (i.e., the pressures to be truthful and informative).
In addition to the uncertainty over the comparison class $c$, the pragmatic listener also have uncertainty over the value of the degree $x$ (e.g., the height of the person) and the semantic threshold variable $\theta$ as in @Lassiter2013.

\begin{align}
L_{1}(x, c, \theta \mid u) &\propto S_{1}(u \mid x, c, \theta) \cdot P(x \mid c_{sub}) \cdot P(c) \cdot P(\theta) \label{eq:L1}\\
S_{1}(u \mid x, c, \theta) &\propto \exp{(\alpha_1 \cdot \ln {L_{0}(x \mid u, c, \theta)})} \label{eq:S1}\\
L_{0}(x \mid u, c, \theta) &\propto {\delta_{[[u]](x, \theta)} \cdot P(x \mid c)}. \label{eq:L0}
\end{align}

Uncertainty about the value of the degree $x$ is always with respect to a comparison class $c$: $P(x \mid c)$.
That is, the plausible heights of basketball players is different than the plausible heights of people in general.

## Model predictions

As a first test of this pragmatic theory, we consider a simple case:
The comparison class is either a subordinate level category (e.g., basketball players) or a superordinate level category (e.g., people *in general*).
Thus, $c \sim \text{UniformDraw}([c_{sub}, c_{super}])$.
We also assume the listener $L_1$ knows the target entity is a member of the subordinate class (i.e., "John is a basketball player."): $P(x \mid c_{sub})$.
Then, the listener $L_1$ tries to uncover the speaker's implicit comparison class $c$ by incorparting what she knows about the subordinate level category (e.g., basketball players) $P(x \mid c_{sub})$ and the superordinate level category (e.g., people in general) $P(x \mid c_{super})$.

The likely comparison class will depend on the details of the prior distributions over the sub- and super-ordinate level classes (e.g., the heights of basketball players and people in general) as well as the polarity of the gradable adjective (i.e., *tall* vs. *short*).
For simplicity, we assume the prior distribution over the degree for the superordinate class is a unit-normal distribution $x_{super} \sim \mathcal{N}(0, 1)$ and explore the model predictions as we vary the mean and standard deviation for the prior of the subordinate class.

<!-- ```{r } -->
<!-- ## to do, pass in the prior params and positive / negative form adjective as arguments to webppl program -->

<!-- model.path <- paste(project.path,"/models/", sep = "") -->

<!-- prior.params <- list( -->
<!--   sub = data.frame(mu = 1, sigma = 0.5),  -->
<!--   super = data.frame(mu = 0 , sigma = 1) -->
<!-- ) -->

<!-- mp.both <- data.frame() -->

<!-- for (u in c("positive", "negative")){ -->

<!--   mp <- webppl( -->
<!--     program_file = paste(model.path, "ccrsa.wppl", sep = ""), -->
<!--     data = list(utt = u, priorParams = prior.params),  -->
<!--     data_var = "paramsFromR", -->
<!--     packages = c( paste(model.path, "node_modules/adjectiveRSA", sep = "") ) -->
<!--   ) -->

<!--   mp.both <- bind_rows(mp %>%  -->
<!--     filter(support == "sub") %>% -->
<!--     mutate(u = u), -->
<!--     mp.both) -->

<!-- } -->


<!-- ``` -->

<!-- ```{r priorModel} -->
<!-- priorModel <- " -->
<!-- var stateParams = { -->
<!--   sub: priorParamsFromR.sub[0], -->
<!--   super: priorParamsFromR.super[0] -->
<!-- }; -->

<!-- var priorModel = function(){ -->
<!--   var subcat = gaussian(stateParams.sub); -->
<!--   var supercat = gaussian(stateParams.super); -->
<!--   return {subcat, supercat} -->
<!-- } -->

<!-- var prior = Infer({method: 'forward', samples: 10000, model: priorModel}) -->
<!-- prior -->
<!-- " -->

<!-- prior.samples <- webppl(priorModel,  -->
<!--                         data = prior.params,  -->
<!--                         data_var = "priorParamsFromR") -->

<!-- prior.tidy <- prior.samples %>%  -->
<!--   rename(subordinate = value.subcat, superordinate = value.supercat) %>% -->
<!--   gather(key, val) %>% -->
<!--   mutate(class = factor(key, levels = c("subordinate", "superordinate"))) -->

<!-- subplt1 <- ggplot(prior.tidy, aes(x = val, y=..scaled.., fill = class))+ -->
<!--   geom_density(alpha = 0.8)+ -->
<!--   scale_fill_brewer(palette = "Set2") + -->
<!--   #guides(fill = F)+ -->
<!--   ggtitle("Priors on degree")+ -->
<!--   xlab("Degree")+ -->
<!--   ylab("Scaled probability") -->
<!-- ``` -->

<!-- ```{r modelSchematics, fig.env = "figure*", fig.pos = "t", fig.width=6.6, fig.height=1.5, fig.align='center', set.cap.width=T, num.cols.cap=2, fig.cap = "Model predictions"} -->

<!-- subplt2 <- ggplot(mp.both %>% -->
<!--                     mutate(Adjective = factor(u,  -->
<!--                                       levels = c( "negative", "positive"))),  -->
<!--                   aes(x = Adjective, y = prob, fill = Adjective))+ -->
<!--   geom_bar(stat= 'identity', position = position_dodge(), color = 'black', alpha = 0.8)+ -->
<!--   scale_fill_brewer(palette = "Set1")+ -->
<!--   geom_hline(yintercept = 0.5, lty = 3)+ -->
<!--   xlab("Inferred comparison class")+ -->
<!--   ylab("P(Sub Class Interpretation)")+ -->
<!--   ggtitle("RSA listener model")+ -->
<!--   theme(text = element_text(size = 10)) -->

<!-- grid.arrange(subplt1, subplt2, nrow = 1) -->
<!-- ``` -->

We see that as the mean of the subordinate prior becomes greater than the superordinate prior, positive form adjectives (e.g., *tall*, *expensive*, *warm*) will be more likely to imply a superordinate comparison class (e.g., *people in general*), while negative form adjectives (e.g., *short*, *cheap*, *cold*) will imply a subordinate class (e.g., *basketball players*).
The opposite pattern is observed for subordinates priors with a mean substantially lower than the superordinate prior.
We test these predictions in our first experiment.

# Experiment 1: Comparison class inference

In this experiment, we test whether the level of abstractness of the comparison class \textit{c} (subordinate vs. superordinate category) depends upon the subordinate category that target entity is a member of. 

## Methods

### Participants

<!--
Note: We are collecting N = 150 for each experiment. Because each participant reads have of the prompts, we expect about n = 75 per item. Simulations reveal that with n = 75, a 95% CI will have maximum width of about 0.2 - 0.25. Note that maximum width occurs when true probability is 0.5.
-->

We recruited 264 participants from Amazon Mechanical Turk. 
Two participants were excluded for failing the catch trial, leaving a total number of 262 participants.
Participation was restricted to those with U.S. IP addresses and who had at least a 95% work approval rating. 
The experiment took about five minutes and participants were compensated $0.50 for their work. 

```{r, eval = F, echo=F}
## simulations to determine width of 95% CI for 2AFC data assuming different sample sizes and true binomial probabilities

### not written efficiently... will take ~10 minutes to run
n_participants <- c(50, 75, 100)
true_probs <- c(0.1, 0.3, 0.5)
simulations <- data.frame()
for (n in n_participants){
  for (p in true_probs){
    for (i in seq(1, 25)){
      simulations <- bind_rows(simulations,
            bind_rows(data.frame(label = c("a"),
                       response = rbinom(n =n, size = 1, prob = p)),
                data.frame(label = c("b"),
                           response = rbinom(n =n, size = 1, prob = p))) %>%
            group_by(label) %>%
            multi_boot_standard(column = "response") %>%
            mutate(width = ci_upper - ci_lower) %>%
          ungroup() %>%
          summarize(w = mean(width)) %>%
            mutate(n = n, p = p, i=i)
      )
    }
  }
}

ggplot(simulations, aes(x = w))+
 geom_histogram()+
 facet_grid(n~p)

```

### Materials

We tested our hypothesis using positive- and negative-form gradable adjectives describing five scales: price (*expensive*, *cheap*), temperature (*warm*, *cold*), duration (*long*, *short*), height (*tall*, *short*), and weight (*heavy*, *light*).
Each scale was paired with a superordinate category: kitchen appliances (price), weather (temperature), things you watch online (duration), people (height), and produce (weight).
For each superordinate category, we used three subordinate categories that aimed to be situated near the high-end (e.g., *basketball player*), low-end (e.g., *gymnast*), and intermediate (e.g., *soccer player*) of the degree scale (here, height).
This resulted in 30 unique events (\{3 subordinate categories\} x \{5 scales\} x \{2 adjective forms (positive and negative)\}; see Table 1 for full listing).

### Procedure

On each trial, participants were given a context sentence to introduce the subordinate category (e.g., *Gary finds a music video to watch online with his friend.*).
This was followed by an adjective sentence, which predicated a positive- or negative-form gradable adjective over the item (e.g., *Gary says to his friend, "It's short."*). 
Participants were asked: "What do you think Gary meant?" and given two options rephrasing the adjective sentence with an explicit comparison class of either the subordinate or superordinate category: 
\begin{enumerate*}
\item "The music video is short relative to music videos" (subordinate comparison class) or
\item "The music video is short relative to things you watch online" (superordinate comparison class). 
\end{enumerate*}

```{r echo = F, eval =F, results = "asis", fig.env = "table*", fig.pos = "h", fig.width = 4, fig.height = 2, fig.align = "center", set.cap.width = T, num.cols.cap = 2, fig.cap = "This image spans both columns. And the caption text is limited to 0.8 of the width of the document."}


height_sub = c("professional basketball player, professional gymnast, professional soccer player")
height_super = c("people")
price_sub = c("dishwasher, oven, toaster")
price_super = c("kitchen appliances")
temperature_sub = c("Fall in Maryland, Summer in Maryland, Winter in Maryland")
temperature_super = c("days of the year in Maryland")
time_sub = c("video of a cute animal, music video, movie")
time_super = c("things you watch online")
weight_sub = c("apple, grape, watermelon")
weight_super = c("produce")

scales = c("Height", "Price", "Temperature", "Time", "Weight")
sub = c(height_sub, price_sub, temperature_sub, time_sub, weight_sub)
super = c(height_super, price_super, temperature_super, time_super, weight_super)

table1 = data.frame(Scale = scales, Subordinate = sub, Superordinate = super)
table1 <- xtable(table1)
print(table1, type = "latex", include.rownames = FALSE, tabular.environment = "tabularx", width = "\\textwidth", floating.environment = "table*")
```


\begin{table*}
\centering
\begin{tabular}{lll}
  \hline
Scale & Subordinate & Superordinate \\
  \hline
Height & professional basketball player, professional gymnast, professional soccer player & people \\
  Price & dishwasher, oven, toaster & kitchen appliances \\
  Temperature & Fall in Maryland, Summer in Maryland, Winter in Maryland & days of the year \\
  Time & video of a cute animal, music video, movie & things you watch online \\
  Weight & apple, grape, watermelon & produce \\
   \hline
\end{tabular}
\caption{Items used in experiments}
\label{tab:1}
\end{table*}

### Procedure

Half of participants completed trials where the superordinate category was previously introduced ("Supercat mention"), possibly to make the superordinate paraphrase more likely.
For example, "Gary is searching for things to watch online with their friend and finds a music video."

Each participant saw 15 trials: one for each subordinate class paired with either the positive or negative form adjective. 
Participants never rated the same subordinate class for both positive and negative form adjectives and back-to-back trials involved different scales to avoid fatigue.
The experiment can be viewed in full at http://stanford.edu/~mtessler/comparison-class/experiments/class-elicitation-2afc.html.

## Results

```{r cache=T} 
data.path <- paste(project.path, "data/classElicitation-1/", sep = "")

d.catch <- read.csv(paste(data.path, "class-elicitation-full-catch_trials.csv", sep = ""))
d.catch <- d.catch %>% 
  mutate(pass = response == "relative to other buildings") %>%
  select(workerid, pass)

d <- read.csv(paste(data.path, "class-elicitation-full-trials.csv", sep = ""))
d.tidy <- left_join(d, d.catch) %>%
  filter(pass) %>%
  mutate(subResponse = ifelse(paraphrase == "super", 0, 1)) 

df.summary <- d.tidy %>%
  group_by(condition, strength, target, degree, adjective, form, sub_category, super_category) %>%
  multi_boot_standard(column = "subResponse")
```

```{r expt1results, fig.env = "figure*", fig.pos = "t", fig.width=6.6, fig.height=4, fig.align='center', set.cap.width=T, num.cols.cap=2, fig.cap = "Experiment 1 results. Comparison class judgments for 30 items on 5 scales."}


df.summary %>%
  ungroup() %>%
  mutate(sub_category = factor(sub_category, levels = sub_category[order(strength)]),
         condition = factor(condition, levels = c("context","contextWithSuper"),
                            labels=c("Bare", "Supercat mention"))) %>%
  ggplot(., 
         aes( x = sub_category, y = mean, ymin = ci_lower, ymax = ci_upper, group = form, fill = form ) )+
  geom_bar(stat = 'identity', position = position_dodge(), alpha = 0.8)+
  geom_errorbar(position = position_dodge())+
  facet_grid(condition~degree, scales = 'free')+
  ylim(0, 1) + 
  theme(axis.text.x = element_text(angle = 90) )+
  scale_fill_brewer(palette = 'Set1')+
  ylab("Proportion Subordinate Comparison Class")+
  xlab("Item category")+
  theme(axis.title.y = element_text(size = 10),
        strip.text = element_text(size = 8))
```

<!-- ```{r glm1, cache = T} -->
<!-- rs.glmm <- glmer(data = d.tidy, subResponse ~ form*strength +  -->
<!--                    (1 + form | degree) +  -->
<!--                    (1 + form | sub_category) +  -->
<!--                    (1 | workerid),  -->
<!--                  family = 'binomial') -->
<!-- summ.rs.glmm <- summary(rs.glmm) -->
<!-- ``` -->

On each trial, the participant was given an utterance with a gradable adjective and asked which comparison class (subordinate vs. superordinate) the speaker meant.
Figure 2 shows the proportion of participants choosing the subordinate category for each item.
We see considerable variability both within- and across- scales.
Within each scale, we see the predicted interaction: For items that are expected to fall high on the scale (basketball players, watermelons, Summer days, dishwashers, movies), positive form adjectives tend to elicit superordinate comparison classes while negative form adjectives tend to elicit subordinate comparison classes (i.e., a basketball player is *tall* relative to other people, but *short* relative to the basketball players).
For items that are expected to fall low on the scale (gymnasts, grapes, Winter days, bottle openers, videos of cute animals), the opposite effect is observed (i.e., a video of a cute animal is *short* relative to other things you watch online, but *long* relative to other videos of cute animals).

To assess the strength of this effect, we used a generalized linear mixed effects model with main effects of adjective form (positive vs. negative) and the *a priori* judgment by the first author of the relative position on the scale of the sub-category (i.e., sub-category item expected to be low, high, or in the middle of the scale), and of critical theoretical interest, the interaction between these two form.
In addition, we included in the model by-participant random effects of intercept, by-degree (temperature, weight, etc...) random effects of intercept and form, and by-subordinate category random effects of intercept and form to predict the probability with which participants would choose the subordinate category as the paraphrase.
As predicted, \red{it's all significant}.


# Experiment 2: Vague speakers

In this experiment, we would like to elicit the priors over the subordinate categories. We use a two-alternative forced choice paradigm to collect peoples' prior belief on the properties of certain items.

\begin{align}
S_{2}(u \mid c_{sub}) &\propto \exp{(\alpha_2 \cdot \ln{L_1(x, \theta \mid u, c_{super})})} \label{eq:S2} \\
L_{1}(x, \theta \mid u, c_{super}) &\propto S_{1}(u \mid x, c_{super}, \theta) \cdot P(x \mid c_{sub}) \cdot P(\theta) \label{eq:L1a}
\end{align}

## Methods

### Participants

We recruited 100 participants from Amazon Mechanical Turk. Five participants were excluded due to failing the catch trial. Participation was restricted to those with U.S. IP addresses and who had at least a 95% work approval rating. On average, the experiment took five minutes and participants were compensated $0.50 for their work. (Include simulation bit here for n = 100?)

### Materials

We created 30 events using positive- and negative-form gradable adjectives from five different measureable scales, each conditioned with three context sentences. The scales we derived adjectives from were height, price, temperature, time, and weight. Three items were assigned to each scale, where each item was its own subordinate category and all had a common superordinate category.

### Procedure

For each event, participants see a context sentence where a subordinate item is presented. They are then asked if 
For example, one event read: "Alicia picks up an apple." "Do you think the apple would be light relative to other produce?" 

## Results

# Discussion

\begin{enumerate}
\item Speaker knowledge: If you're a basketball scout, and you say of a player that "He is tall." it means "Tall relative to basketball players"
\item QUD: If we're deciding what to do on Friday night, you say "The opera is expensive" it means "Expensive relative to other things we could do on Friday night"
\item Hyperbole / *normative* comparison classs: If we listen to a lecture, and you say "That *was long*.", it means "Long relative to how long I think it should have been." If we go out for pasta, and you ask how it was, and I say "it was expensive" it means "expensive relative to how much it should have been given the quality".
\end{enumerate}

<!--
For general information about authoring in markdown, see **[here](http://rmarkdown.rstudio.com/authoring_basics.html).**

First level headings should be in 12 point , initial caps, bold and
centered. Leave one line space above the heading and 1/4~line space
below the heading.

## Second-Level Headings

Second level headings should be 11 point , initial caps, bold, and
flush left. Leave one line space above the heading and 1/4~ line
space below the heading.

### Third-Level Headings

Third-level headings should be 10 point , initial caps, bold, and flush
left. Leave one line space above the heading, but no space after the
heading.

# Formalities, Footnotes, and Floats

Use standard APA citation format. Citations within the text should
include the author's last name and year. If the authors' names are
included in the sentence, place only the year in parentheses, as in
[-@NewellSimon1972a], but otherwise place the entire reference in
parentheses with the authors and year separated by a comma
[@NewellSimon1972a]. List multiple references alphabetically and
separate them by semicolons [@ChalnickBillman1988a; @NewellSimon1972a].
Use the et. al. construction only after listing all the authors to a
publication in an earlier reference and for citations with four or
more authors.

For more information on citations in R Markdown, see **[here](http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html#citations).**

## Footnotes

Indicate footnotes with a number\footnote{Sample of the first
footnote.} in the text. Place the footnotes in 9 point type at the
bottom of the page on which they appear. Precede the footnote with a
horizontal rule.\footnote{Sample of the second footnote.}

## Figures

All artwork must be very dark for purposes of reproduction and should
not be hand drawn. Number figures sequentially, placing the figure
number and caption, in 10 point, after the figure with one line space
above the caption and one line space below it. If necessary, leave extra white space at
the bottom of the page to avoid splitting the figure and figure
caption. You may float figures to the top or bottom of a column, or
set wide figures across both columns.

## Two-column images

You can read local images using png package for example and plot
it like a regular plot using grid.raster from the grid package.
With this method you have full control of the size of your image. **Note: Image must be in .png file format for the readPNG function to work.**

You might want to display a wide figure across both columns. To do this, you change the `fig.env` chunk option to `figure*`. To align the image in the center of the page, set `fig.align` option to `center`. To format the width of your caption text, you set the `num.cols.cap` option to `2`.

```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=4, fig.height=2, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "This image spans both columns. And the caption text is limited to 0.8 of the width of the document."}
img <- png::readPNG("figs/walrus.png")
grid::grid.raster(img)
```

## One-column images

Single column is the default option, but if you want set it explicitly, set `fig.env` to `figure`. Notice that the `num.cols` option for the caption width is set to `1`.

```{r image, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=2, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "One column image."}
img <- png::readPNG("figs/lab_logo_stanford.png")
grid::grid.raster(img)
```


## R Plots

You can use R chunks directly to plot graphs. And you can use latex floats in the
fig.pos chunk option to have more control over the location of your plot on the page. For more information on latex placement specifiers see **[here](https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions)**

```{r plot, fig.env="figure", fig.pos = "H", fig.align = "center", fig.width=2, fig.height=2, fig.cap = "R plot" }
x <- 0:100
y <- 2 * (x + rnorm(length(x), sd = 3) + 3)

ggplot2::ggplot(data = data.frame(x, y),
       aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm")
```


## Tables

Number tables consecutively; place the table number and title (in
10 point) above the table with one line space above the caption and
one line space below it, as in Table 1. You may float
tables to the top or bottom of a column, set wide tables across both
columns.

You can use the xtable function in the xtable package.

-->

# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.




<!--
## Experiments

See all the items [here](http://stanford.edu/~mtessler/comparison-class/experiments/js/examples-2.js), or see the listener experiment (with all the items) [here](http://stanford.edu/~mtessler/comparison-class/experiments/listener-1.html).

\begin{enumerate}

\item \textbf{Stimuli}
  \subitem Currently we have 40 unique items, made up of 6 scales x 2 (positive and negative form adjectives)
  \subitem Note we are using temperature that is perceived both through touching and temperature that is perceived in the whole body (touching objects vs. the weather). For the weather, it is really about speaker's beliefs--their own subjective comparison class (e.g., John think its cold because he's from Southern California).
  \subitem 3 or 4 contexts by item (different targets of reference)
  \subitem Example: Gary buys a [television, book, iPhone] and picks it up. He says, ``This is heavy.''

\item \textbf{Comparison class elicitation}
  \subitem We have run a pilot using the paraphrase technique with the helper phrases "for a" and "relative to." "For a" produced more reliable results, so we'll use that going forward.
  \subitem Example: Gary buys a television and picks it up. He says, ``This is heavy.''
  \subitem \textbf{Dependent measure}: What do you think Gary meant? ``This is heavy \textbf{for a} ...'' (fill in the blank)
  \subitem \textbf{Data analysis}: Take modal response, or top 2, and use those in the prior elicitation task.

\item \textbf{Prior elicitation}
  \subitem This will refer to the comparison class.
  \subitem \textbf{Dependent measure}: What do you think is the weight of a television? Provide number and units (units from a set from a drop-down menu).

\item \textbf{Language understanding (Listener task)}
  \subitem Same stimuli as the comparison class elicitation.
  \subitem \textbf{Dependent measure}: same as prior elicitation.

\end{enumerate}
# Model

uRSA with lifted variable.
Plug in different priors.

# Data anlaysis

Probably something similar to the habituals paper.
Assume log-normals, do BDA on the prior data.

# Current concerns

\begin{enumerate}

\item In the Justine-way of doing these kinds of experiments, for the prior elicitation, we would have the same context sentence (``Gary buys a television and picks it up.'') without the adjective sentence, and ask about the likely weight.
What does our intermediate comparison class elicitation step buy us?

\item For many of the our items, it seems that the comparison class we're going to get is the class of the target objects (e.g., televisions, cups of coffee, weather in Southern California)... I have the feeling that some items (e.g., a 30 year old who is tall) have more general classes (e.g., tall for an adult, for a woman) while others (e.g., a 4 year old boy who is tall) have more specific classes (i.e., tall for a 4 year old). This example may be unique / idiosyncractic, but also may suggest that the comparison class needs to be a relative homogeneous category (e.g., 4 year olds are really different than 2 year olds, whereas 30 vs. 28 year olds not so much). If this is interesting, should we try to move our stimuli more in that direction? Any thoughts?

\end{enumerate}

-->

# References

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in}
\setlength{\leftskip}{0.125in}
\noindent
