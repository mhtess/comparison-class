---
title: "Warm (for winter): Comparison class understanding in vague language"
bibliography: [comparison-class.bib, library.bib]
csl: "apa6.csl"
document-params: "10pt, letterpaper"
header-includes:
  - \usepackage{tabularx}
  - \usepackage{multicol}
  - \usepackage{wrapfig}
  #- \usepackage{stfloats}

author-information: >
  \author{{\large \bf Michael Henry Tessler$^1$} (mtessler@stanford.edu) \and {\large \bf Michael Lopez-Brau$^2$} (lopez\_mic@knights.ucf.edu) \\
  {\large \bf Noah D. Goodman$^1$} (ngoodman@stanford.edu) \\
  $^1$Department of Psychology, Stanford University,
  $^2$Department of Electrical \& Computer Engineering, University of Central Florida}

abstract:
    "Vague language often communicates relative meanings.
    For example, the gradable adjective *expensive* conveys that the price of some object is high relative to some comparison class---a set of entities against which the target object is implicitly compared.
    Comparison classes are known to be central to adjectival interpretation and even have been used to explain other kinds of vague language (e.g., generic language).
    Yet little is known about where comparison classes come from.
    Here, we propose the resolution of a comparison class in context is a pragmatic inference driven by world knowledge.
    First, we introduce a Rational Speech Act model that makes a novel prediction about the level of abstraction of the comparison class (e.g., expensive relative to a subordinate vs. superordinate class) and validate this prediction using a paraphrase experiment.
    Quantative predictions of the model rely upon the structure of relevant world knowledge, which we elicit using a novel technique that avoids participants having to estimate quantities.
    "


keywords:
    "comparison class; pragmatics; Bayesian cognitive model; Bayesian data analysis"

output: cogsci2016::cogsci_paper
---

\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}
\definecolor{Orange}{RGB}{255,153,0}

\newcommand{\red}[1]{\textcolor{Red}{#1}}  
\newcommand{\ndg}[1]{\textcolor{Green}{[ndg: #1]}}  
\newcommand{\mht}[1]{\textcolor{Blue}{[mht: #1]}}  
\newcommand{\mlb}[1]{\textcolor{Orange}{[mlb: #1]}}

```{r global_options, include=FALSE}
rm(list=ls())

# set local path for the repo
if (grepl("michael", getwd())) {
  project.path <- "/media/michael/Data/Desktop/comparison-class/"
} else if (grepl("mht", getwd())) {
  project.path <- "/Users/mht/Documents/research/comparison-class/"
} else {
  project.path <- "Noah's path"
}

knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop=F, fig.pos="tb", fig.path='figs/', echo=F, warning=F, cache=F, message=F, sanitize=T)
```

```{r, libraries}
library(png)
library(grid)
library(gridExtra)
library(tidyverse)
library(xtable)
library(rwebppl)
library(langcog)
library(coda)
library(lme4)
library(lmerTest)

estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}

HPDhi<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

HPDlo<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}
```

```{r}
theme_paper <- function(base_size = 12, base_family = "Helvetica") {
  theme(
    line = element_line(colour = "black", size = 0.5, linetype = 1, lineend = "butt"),
    rect = element_rect(fill = "black", colour = "black", size = 0.5, linetype = 1),
    text = element_text(family = base_family, face = "plain",
                        colour = "black", size = base_size,
                        hjust = 0.5, vjust = 0.5, angle = 0, lineheight = 0.9,
                        margin = margin(2,1,3,1), debug = FALSE),
    axis.text = element_text(size = 12, colour = "black", margin=margin(1,1,1,1)),
    strip.text = element_text(size = 12, colour = "black", margin=margin(1,1,4,1)),

    axis.line = element_blank(),
    axis.ticks = element_line(colour = "black", size = 0.2),
    axis.title = element_text(colour = "black", size = 16),
    axis.ticks.length = unit(0.3, "lines"),

    legend.background = element_rect(colour = "white", fill = 'white'),
#    legend.margin =      unit(0.2, "cm"),
    legend.key = element_rect(fill = "white", colour = "white"),
    legend.key.size = unit(1.2, "lines"),
    legend.key.height = NULL,
    legend.key.width = NULL,
    legend.text = element_text(size = rel(1.2), colour = "black"),
    legend.text.align = NULL,
    legend.title = element_text(size = rel(1.2), face = "bold", hjust = 0, 
                                colour = "black"),
    legend.title.align = NULL,
    legend.position = "right",
    legend.direction = "vertical",
    legend.justification = "center",
    legend.box = NULL,

    panel.background =   element_rect(fill = "white", colour = NA),
    panel.border =       element_rect(fill = NA, colour = "black"),
    panel.grid.major =   element_line(colour = "white", size = 0.2),
    panel.grid.minor =   element_line(colour = "white", size = 0.5),
    panel.spacing =       unit(0.5, "lines"),

    strip.background =   element_rect(fill = "white", colour = "white"),
    strip.text.x =       element_text(),
    strip.text.y =       element_text(angle = -90),

    plot.background =    element_rect(colour = "white", fill = "white"),
    plot.title =         element_text(size = rel(1.2)),
    plot.margin =        unit(c(1, 1, 0.5, 0.5), "lines"),

    complete = TRUE
  )
}
theme_set(theme_bw())
```

# Introduction

When it's warm outside it's still useful to know whether we'rein July or January.
A warm day in July could be 80 degrees while a warm one in January might not get above 60.
The predicate "warm" is vague, in the sense that there are borderline cases of being warm, interlocutors can faultlessly disagree as to whether or not it is warm, and what it means to be warm is relative to a relevant *comparison class*.

Comparison classes are uncontroversial in the study of vague language. 
Even, 2 or 3 year olds understand that the meaning of the words "big" and "little" depend upon the context: 
An object can be "big" relative to other objects in a scene (i.e., its physical context), 
an object can be "big" relative to the size it should be for completing its intended function, 
or an object can just be "big" [or, big relative to an implicit comparison class; @Ebeling1994]. \mht{$\leftarrow$ not sure if this is the best evidence for my topic sentence}
Adult judgments of the felicity for gradable adjectives like "dark" or "light", "short" or "tall" depend upon fine-grained details of the statistics of the comparison class [@Solt2012; @Qing2014].
It is not well undertsood, however, how a comparison class becomes contextually-salient. 
The problem is an obvious one because any particular object of discourse can be conceptualized or categorized in multiple ways, giving rise to multiple possible comparison classes. 
A warm day in January could be warm for the time of year (i.e., *warm relative to other days in January*^[
also, *in this location*, where the speaker is]
or it could just be warm (i.e., *warm relative to any day of the year*)

We propose that the selection of a comparison class with respect to which vague languages gets interpreted is arrived upon through a pragmatic reasoning process.
<!--We propose that vague language gets interpreted with respect to a particular comparison class via a pragmatic reasoning process.-->
We introduce a novel language understanding model that can resolve uncertainty about the likely reference class alongside the uncertainty known to be important for modeling vague language understanding (namely, uncertainty about the world and uncertainty about the precise meaning of the vague speech act).
From this model, we derive two qualitative predictions which we test in an experiment designed to elicit the likely comparison class.
\mht{maybe sketch out the two predictions, if they can be explained straightforwardly}.
Quantitative predictions of the model will depend on the quantitative details of the relevant world knowledge.
This could be measured by having participants estimate quantities or give likelihood judgments [@FrankeEtAl2016].
Instead, we build a Bayesian data analytic model to introduce uncertainty over these paremters and resolve the uncertainty by asking similar natural language questions in the domains of inquiry. 
This represents a new possibility for Bayesian models of cognition and language: By building more complete and coherent models of language, additional parameters can be estimated reliably by asking the model to give multiple judgments on different tasks.

<!--
"It's warm outside" when heard in July means something very different than when heard in January.
Warm is vague,
When heard in Winter, "It's warm" might mean it's 50 degrees; in Summer, maybe 80.
-->

# Computational model

Adjectives like *warm* and *cold*, *cheap* and *expensive*,  are vague descriptions of an underlying quantitative scale (e.g., temperature, price).
Contemporary linguistic theories posit that the truth-conditional semantics of such vague utterances are simple thresholds on the measure [@Kennedy2007].
For example, the weather is *warm* if its temperature is greater than some threshold number of degrees.
The crux of such theories rests on the fact that this threshold is derived relative to a *comparison class* of other possible entities.
A day in Winter can be warm *relative to* other days in Winter but probably not warm relative to all days of the year.

The comparison class cam be thought of as a probability distribution over degrees on a scale [@Lassiter2013; @Qing2014a].
We adopt a probabilistic model of adjectival interpretation that is derived from general principles of language understanding [@Lassiter2013].
The model is an extension of the Rational Speech Act theory of language understanding [for a review see @Goodman2016] and uses a simple threshold semantics for adjectives $[[u]]: x > \theta$ (i.e., warm means greater than some temperature).
Uncertainty is placed over the threshold $\theta \sim \text{Uniform}(0, 1)$ and is resolved by the pragmatic listener reasoning about the likely values of the degree $P(x)$, given a comparison class.
Such a model reasons about a comparison class, but doesn't have anything to say about where the comparison class comes from.
The issue of the comparison class touches upon not only gradable adjectives, but in fact any language that can be modeled with an underspecified threshold criterion including generic language [@Tessler2016] and vague quantifiers [@SchollerFranke2015].
How does a listener settle on the appropriate comparison class, when none is specifically articulated by the speaker?

<!--
In this paper, we argue that pragmatic principles can be used to resolve the comparison class in context.
We extend the Rational Speech Act theory of @Lassiter2013 to include uncertainty over the comparison class.
We test this model in a simplified setting, where the comparison class could either be a subordinate or superordinate class (e.g., toasters vs. kitchen appliances).
In this simplified setting, we find the model predicts the level of abstraction (sub vs. superordinate) of a comparison class depends upon the quantitative details of the relevant prior beliefs (e.g., the prices of toasters vs. the prices of kitchen appliances in general).
We test this prediction using a paraphrase experiment (Expt. 1).
Quantitative predictions for Bayesian models (including this one) depend upon the quantitative details of listeners' prior distributions over various scales.
Because it may be difficult for human participants to estimate quantities accurately, we introduce a novel Bayesian data-analytic technique for learning about listeners' prior belief distributions using convergent language understanding tasks.
-->

We elaborate the Rational Speech Act model for vague language understanding [@Lassiter2013; @Lassiter2015] by introducing uncertainty over the class of entities against which the target entity is compared--- the comparison class $c$.
This uncertainty is posited at the level of the pragmatic listener, and thus is resolved using pragmatic reasoning (i.e., the pressures to be truthful and informative).
In addition to the uncertainty over the comparison class $c$, the pragmatic listener also have uncertainty over the value of the degree $x$ (e.g., the temperature of the day) and the semantic threshold variable $\theta$ as in @Lassiter2013.
Uncertainty about the value of the degree $x$ is always with respect to a comparison class $c$: $P(x \mid c)$.

As a first test of this idea, we consider a simplified case where the comparison class can either be subordinate or a superordinate categorization (e.g., *warm* relative to days in Winter or relative to all days of the year). 
The listener is aware that the target entity is a member of the subordinate class (e.g., aware that it is winter) and hence draws likely values of the degree from the subordinate class prior: $P(x \mid c_{sub}$.

\begin{align}
L_{1}(x, c, \theta \mid u) &\propto S_{1}(u \mid x, c, \theta) \cdot P(x \mid c_{sub}) \cdot P(c) \cdot P(\theta) \label{eq:L1}
\end{align}

The listener believes the speaker had some intended comparison class in mind, as well as some degree he was trying to communicate and some threshold he was using: $S_{1}(u \mid x, c, \theta)$.
The speaker's utility function is a standard information-theoretic utility in which the speaker gains higher utility by decreasing a *literal listener*'s surprisal [@Goodman2013].
The speaker is assumed to be soft-max rational with degree of rationality governed by $\alpha_1$. 

\begin{align}
S_{1}(u \mid x, c, \theta) &\propto \exp{(\alpha_1 \cdot [\ln {L_{0}(x \mid u, c, \theta)} - \text{cost}(u)])} \label{eq:S1}\\
L_{0}(x \mid u, c, \theta) &\propto {\delta_{[[u]](x, \theta)} \cdot P(x \mid c)}. \label{eq:L0}
\end{align}

In addition, the speaker has a cost function associated with the relative complexity of alternative utterances $\text{cost}(u)$.
In this paper, we use a speaker model that considers 3 different utterances in each of the positive and negative forms, for a total of six utterances: the gradable adjective with the implicit comparison class (e.g. "It is \{warm, cold\}") and each of two explicit comparison classes (e.g., "It is \{warm, cold\} relative to other days in Winter.", "It is \{warm, cold\} relative to other days of the year.").
We are interested in the behavior of the model with the underspecified utterance (e.g., "It is warm").
<!--
When the pragmatic listener hears this utterance, she must reason about which comparison class the speaker had in mind.
When the speaker produces the unambiguous alterantive utterance (e.g., "It is relative to other days in Winter."), the listener 
-->
In the following section, we explore the predictions of this model.

## Model simulations

```{r wpplHelpers}
webpplHelpers <- '
var round = function(x){
  return Math.round(x*10)/10
}

var distProbs = function(dist, supp) {
  return map(function(s) {
    return Math.exp(dist.score(s))
  }, supp)
}

var KL = function(p, q, supp) {
  var P = distProbs(p, supp), Q = distProbs(q, supp);
  var diverge = function(xp,xq) {
    return xp == 0 ? 0 : (xp * Math.log(xp / xq) );
  };
  return sum(map2(diverge,P,Q));
};

var exp = function(x){return Math.exp(x)}
'
```

```{r rsaPrior}
priorForRSA <- '
var binParam = 3;

var stateParams = {
  sub: paramsFromR.priorParams.sub[0],
  super: paramsFromR.priorParams.super[0]
};

var stateVals = map(
  round,
  _.range(stateParams.super.mu - 3 * stateParams.super.sigma,
          stateParams.super.mu + 3 * stateParams.super.sigma,
          stateParams.super.sigma/binParam)
);

var stateProbs = {
  sub: map(function(s){
    Math.exp(Gaussian(stateParams.sub).score(s))+
    Number.EPSILON
  }, stateVals),
  super: map(function(s){
    Math.exp(Gaussian(stateParams.super).score(s))+
    Number.EPSILON
  }, stateVals)
};

var statePrior = {
  sub: Infer({
    model: function(){ return categorical({vs: stateVals, ps: stateProbs.sub}) }
  }),
  super: Infer({
    model: function(){ return categorical({ vs: stateVals, ps: stateProbs.super}) }
  })
};
'
```

```{r rsaLanguage}
languageForRSA <- '
var thresholdBins ={
  positive: map(function(x){
    return  x - (1/(binParam*2));
  }, sort(statePrior.super.support())),
  negative: map(function(x){
    return  x + (1/(binParam*2));
  }, sort(statePrior.super.support()))
};

var thresholdPrior = cache(function(form){
  return Infer({
    model: function() { return uniformDraw(thresholdBins[form]) }
  });
});

var utterances = {
  positive: ["positive_Adjective",
             "negative_Adjective",
             "silence_silence",
             "positive_sub",
             "positive_super",
             "negative_sub",
             "negative_super"],
  negative: ["positive_Adjective",
             "negative_Adjective",
             "silence_silence",
             "positive_sub",
             "positive_super",
             "negative_sub",
             "negative_super"]
};

// explicit comparison class is twice as expensive; silence is cheap
var utteranceCosts = [1, 1, 0.5, 2, 2, 2, 2];
// uniform costs
// var utteranceCosts = [1, 1, 1, 1, 1, 1, 1];
var utteranceProbs = map(function(c) {return exp(-c)}, utteranceCosts);
var utterancePrior = cache(function(form){
  return Infer({
    model: function() {
      return categorical({
        vs: utterances[form],
        ps: utteranceProbs
      })
    }
  })
});

var meaning = function(utterance, state, thresholds) {
  utterance == "positive" ? state > thresholds.positive ? flip(0.9999) : flip(0.0001) :
  utterance == "negative" ? state < thresholds.negative ? flip(0.9999) : flip(0.0001) :
  true
}

'
```

```{r ccRSA}
ccrsa <- '
// webppl ccrsa.wppl --require adjectiveRSA
var classPrior = Infer({
  model: function(){return uniformDraw(["sub", "super"])}
});

var alphas = {s1: 3, s2: 1};

var literalListener = cache(function(u, thresholds, comparisonClass) {
  Infer({model: function(){
    var cc = u.split("_")[1] == "Adjective" ?
        comparisonClass :
    u.split("_")[1] == "silence" ?
        comparisonClass :
    u.split("_")[1]    

    var state = sample(statePrior[cc]);
    var utterance = u.split("_")[0]
    var m = meaning(utterance, state, thresholds);
    condition(m);
    return state;
  }})
}, 10000)

var speaker1 = cache(function(state, thresholds, comparisonClass, form) {
  Infer({model: function(){
    var utterance = sample(utterancePrior(form))
    var L0 = literalListener(utterance, thresholds, comparisonClass)
    factor( alphas.s1 * L0.score(state) )
    return utterance
  }})
}, 10000)

var pragmaticListener = function(form) {
  Infer({model: function(){
    var utterance = form + "_Adjective";
    var comparisonClass = sample(classPrior);
    var state = sample(statePrior["sub"]);
    var thresholds = {
      positive: sample(thresholdPrior("positive")),
      negative: sample(thresholdPrior("negative"))
    };
    var S1 = speaker1(state, thresholds, comparisonClass, form);
    observe(S1, utterance);
    return comparisonClass
  }})
}

pragmaticListener(paramsFromR.utt[0])
'
```

```{r runModel, cache = T}


sub.prior.params <- c( list( sub = data.frame(mu = 1, sigma = 0.5) ),
                   list( sub = data.frame(mu = 0, sigma = 0.5) ),
                   list( sub = data.frame(mu = -1, sigma = 0.5) ))

mp.both <- data.frame()

fullModel <- paste(webpplHelpers, priorForRSA, languageForRSA, ccrsa, sep = "\n")

for (p in sub.prior.params){
  prior.params <- list(super = data.frame(mu = 0, sigma = 1), sub = p)

  for (u in c("positive", "negative")){
  
    mp <- webppl(
      program_code = fullModel,
      data = list(utt = u, priorParams = prior.params),
      data_var = "paramsFromR"
    )

    mp.both <- bind_rows(mp %>%
      filter(support == "sub") %>%
      mutate(u = u, sub_mu = p["mu"][[1]], sub_sigma = p["sigma"][[1]]),
      mp.both)
  }
  #print(p$mu)
}

```

```{r priorModel, cache = T}
priorModel <- "
var stateParams = {
  sub: priorParamsFromR.sub[0],
  super: priorParamsFromR.super[0]
};

var priorModel = function(){
  var subcat = gaussian(stateParams.sub);
  var supercat = gaussian(stateParams.super);
  return {subcat, supercat}
}

var prior = Infer({method: 'forward', samples: 10000, model: priorModel})
prior
"
all.priors <- data.frame()
for (p in sub.prior.params){
  prior.params <- list(super = data.frame(mu = 0, sigma = 1), sub = p)

  prior.samples <- webppl(priorModel,
                          data = prior.params,
                          data_var = "priorParamsFromR")

  prior.tidy <- prior.samples %>%
    rename(subordinate = value.subcat, superordinate = value.supercat) %>%
    gather(key, val) %>%
    mutate(class = factor(key, levels = c( "superordinate", "subordinate")),
           sub_mu = p["mu"][[1]], sub_sigma = p["sigma"][[1]])
  
  all.priors <- bind_rows(all.priors, prior.tidy)
  #print(p$mu)
}
```

```{r modelSchematics, fig.env = "figure*", fig.pos = "htb", fig.width=6.6, fig.height=1.5, fig.align='center', set.cap.width=T, num.cols.cap=2, fig.cap = "Left: Three schematic prior distributions over subordinate comparison class (fixing the superordinate comparison class to be a unit-normal distribution). Right: Predicted listener inference for the probability of an intended subordinate comparison class. Model prediction assume an alpha of 3 and a cost of 2 for the explicit comparison class alternatives. The comparison class prior does not favor either the subordinate or superordinate class."}

subplt1 <- ggplot(all.priors, aes(x = val, y=..scaled.., fill = class))+
  geom_density(alpha = 0.8)+
  scale_fill_brewer(palette = "Set2") +
  #guides(fill = F)+
  ggtitle("Priors on degree")+
  xlab("Degree")+
  ylab("Scaled probability")+
  facet_wrap(~sub_mu, nrow = 1)+
  theme(title = element_text(size = 8),
        legend.text = element_text(size = 6),
        axis.text = element_text(size = 6))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))+
  scale_x_continuous(limits = c(-4, 4), breaks = c(-2, 0, 2))


subplt2 <- ggplot(mp.both %>%
                    mutate(Adjective = factor(u,
                                      levels = c("negative", "positive"))),
                  aes(x = sub_mu, y = prob, fill = Adjective))+
  geom_bar(stat= 'identity', position = position_dodge(), color = 'black', alpha = 0.8)+
  scale_fill_brewer(palette = "Set1")+
  geom_hline(yintercept = 0.5, lty = 3)+
  xlab("Subordinate prior mean")+
  ylab("Subordinate interpretation")+
  ggtitle("RSA listener model") +
  theme(title = element_text(size = 8),
        legend.text = element_text(size = 6))+
  scale_y_continuous(limits = c(0, 1), breaks = c(0, 0.5, 1))

grid.arrange(subplt1, subplt2, nrow = 1)
```

As a first test of this pragmatic theory, we consider a simple case:
The comparison class is either a subordinate level category (e.g., days in Winter) or a superordinate level category (e.g., days of the year).
Thus, $c \sim \text{UniformDraw}([c_{sub}, c_{super}])$.
We also assume the listener $L_1$ knows the target entity is a member of the subordinate class (e.g., knows that it's Winter): $P(x \mid c_{sub})$.
Then, the listener $L_1$ tries to uncover the speaker's implicit comparison class $c$ by incorparting what she knows about the subordinate level category (e.g., temperature of days in winter) $P(x \mid c_{sub})$ and the superordinate level category (e.g., temperature of days in general) $P(x \mid c_{super})$.

The likely comparison class will depend on the details of the prior distributions over the sub- and superordinate level classes (e.g., listener's subjective degrees of belief in various temperatures) as well as the polarity of the gradable adjective (i.e., *tall* vs. *short*).
For simplicity, we assume the prior distribution over the degree for the superordinate class is a unit-normal distribution $x_{super} \sim \mathcal{N}(0, 1)$ and explore the model predictions as we vary the mean and standard deviation for the prior of the subordinate class.
<!--
Model schematic. Show "John is tall for a person", "John is tall for a baskebatll player", "John is tall."
-->

For purposes of illustration, consider the three subordinate priors to correspond to temperatures in Winter (-1), Fall (0), and Summer (1) in a part of the world that exhibits seasons (e.g., Maryland, USA), and the superordinate prior to corresponds to temperatures including all days of the year (Figure \ref{fig:modelSchematics}, left).
When the pragmatic listener (Eq. \ref{eq:L1}) is in Winter and hears that "It's warm" (positive adjective), she thinks it's more likely that it's warm relative to days in Winter than when she hear's it cold (Figure \ref{fig:modelSchematics}, right, left-most facet; compare blue vs. red bars).
The model thinks the opposite is true in Summer: A warm day in summer could be warm for summer, or warm for the year, while a cold day in Summer is much more likely to be cold for summer.
In all situations, the model thinks it's more likely than not for the comparison class to be the subordinate category (all bars greater than 0.5).
This comes from the vagueness of adjectives: The meaning of the adjective in context depends upon the prior; a prior with less variance will result in a relatively more precise meaning.
The pragmatic listener overall prefers subordinate comparison classes, though the extent of which is modulated by the relationship of the subordinate to the superordinate class prior.
<!--
We see that as the mean of the subordinate prior becomes greater than the superordinate prior, positive form adjectives (e.g., *tall*, *expensive*, *warm*) will be more likely to imply a superordinate comparison class (e.g., *people in general*), while negative form adjectives (e.g., *short*, *cheap*, *cold*) will imply a subordinate class (e.g., *basketball players*).
The opposite pattern is observed for subordinates priors with a mean substantially lower than the superordinate prior.
We test these predictions in our first experiment.
-->
```{r echo = F, eval =F, results = "asis", fig.env = "table*", fig.pos = "b", fig.width = 4, fig.height = 2, fig.align = "center", set.cap.width = T, num.cols.cap = 2, fig.cap = "This image spans both columns. And the caption text is limited to 0.8 of the width of the document."}

height_sub = c("professional basketball player, professional gymnast, professional soccer player")
height_super = c("people")
price_sub = c("dishwasher, oven, toaster")
price_super = c("kitchen appliances")
temperature_sub = c("Fall in Maryland, Summer in Maryland, Winter in Maryland")
temperature_super = c("days of the year in Maryland")
time_sub = c("video of a cute animal, music video, movie")
time_super = c("things you watch online")
weight_sub = c("apple, grape, watermelon")
weight_super = c("produce")

scales = c("Height", "Price", "Temperature", "Time", "Weight")
sub = c(height_sub, price_sub, temperature_sub, time_sub, weight_sub)
super = c(height_super, price_super, temperature_super, time_super, weight_super)

table1 = data.frame(Scale = scales, Subordinate = sub, Superordinate = super)
table1 <- xtable(table1)
print(table1, type = "latex", include.rownames = FALSE, tabular.environment = "tabularx", width = "\\textwidth", floating.environment = "table*")
```

\begin{table*}
\centering
\begin{tabular}{lll}
  \hline
Scale \{\emph{adjectives}\}& Subordinate & Superordinate \\
  \hline
Height \{\emph{tall}, \emph{short}\} & professional \{basketball player, soccer player, gymnast\} & people \\
  Price \{\emph{expensive}, \emph{cheap}\}& \{dishwasher, toaster, oven\} & kitchen appliances \\
  Temperature \{\emph{warm}, \emph{cold}\}& \{Summer, Fall, Winter\} day in Maryland & days of the year \\
  Time \{\emph{long}, \emph{short}\}& \{movie, music video, video of a cute animal\}  & things you watch online \\
  Weight \{\emph{heavy}, \emph{light}\}& \{watermelon, grape, apple\}  & produce \\
   \hline
\end{tabular}
\caption{Items used in experiments}
\label{tab:1}
\end{table*}

# Experiment 1: Comparison class inference

In this experiment, we test whether the level of abstractness of the comparison class \textit{c} (subordinate vs. superordinate category) depends upon the subordinate category that target entity is a member of.

## Methods

### Participants

<!--
Note: We are collecting N = 150 for each experiment. Because each participant reads have of the prompts, we expect about n = 75 per item. Simulations reveal that with n = 75, a 95% CI will have maximum width of about 0.2 - 0.25. Note that maximum width occurs when true probability is 0.5.
-->

We recruited 264 participants from Amazon Mechanical Turk.
Two participants were excluded for failing the catch trial, leaving a total number of 262 participants.
Participation was restricted to those with U.S. IP addresses and who had at least a 95% work approval rating.
The experiment took about five minutes and participants were compensated $0.50 for their work.

```{r, eval = F, echo=F}
## simulations to determine width of 95% CI for 2AFC data assuming different sample sizes and true binomial probabilities

### not written efficiently... will take ~10 minutes to run
n_participants <- c(50, 75, 100)
true_probs <- c(0.1, 0.3, 0.5)
simulations <- data.frame()
for (n in n_participants){
  for (p in true_probs){
    for (i in seq(1, 25)){
      simulations <- bind_rows(simulations,
            bind_rows(data.frame(label = c("a"),
                       response = rbinom(n =n, size = 1, prob = p)),
                data.frame(label = c("b"),
                           response = rbinom(n =n, size = 1, prob = p))) %>%
            group_by(label) %>%
            multi_boot_standard(column = "response") %>%
            mutate(width = ci_upper - ci_lower) %>%
          ungroup() %>%
          summarize(w = mean(width)) %>%
            mutate(n = n, p = p, i=i)
      )
    }
  }
}

ggplot(simulations, aes(x = w))+
 geom_histogram()+
 facet_grid(n~p)

```

### Materials

We tested our hypothesis using positive- and negative-form gradable adjectives describing five scales: price (*expensive*, *cheap*), temperature (*warm*, *cold*), duration (*long*, *short*), height (*tall*, *short*), and weight (*heavy*, *light*).
Each scale was paired with a superordinate category: kitchen appliances (price), weather throughout the year (temperature), things you watch online (duration), people (height), and produce (weight).
For each superordinate category, we used three subordinate categories that aimed to be situated near the high-end, low-end, and intermediate part of the degree scale (as in Model Prediction section).
This resulted in 30 unique events (\{3 subordinate categories\} x \{5 scales\} x \{2 adjective forms\}; see Table \ref{tab:1} for full listing).

### Procedure

Each participant saw 15 trials: one for each subordinate category paired with either the positive or negative form of its corresponding adjective.
On each trial, participants were given a context sentence to introduce the subordinate category (e.g., *Tanya lives in Maryland and steps outside in Winter.*).
This was followed by an adjective sentence, which predicated either a positive- or negative-form gradable adjective over the item (e.g., *Tanya says to her friend, "It's warm."*).
Participants were asked "What do you think Tanya meant?" and given two options to rephrase the adjective sentence with an explicit comparison class of either the subordinate or superordinate category: 

\begin{itemize}
\item \{She / He / It\} is \textsc{adjective} relative to other \textsc{subordinates} (e.g., \emph{It's warm relative to other days in Winter})
\item \{She / He / It\} is \textsc{adjective} relative to other \textsc{superordinates} (e.g., \emph{It's warm relative to other days of the year})
\end{itemize}

Participants never judged the same subordinate category for both adjective forms (e.g., cold and warm Winter days) and back-to-back trials involved different scales to avoid fatigue.
In addition to all of the above design parameters, half of participants completed trials where an additional sentence introduced the superordinate category at the beginning (e.g., *Tanya lives in Maryland and checks the weather every day.*), with the intention of making the superordinate paraphrase more likely. 
The experiment can be viewed in full at:
\url{http://stanford.edu/~mtessler/comparison-class/experiments/class-elicitation-2afc.html}

## Results

```{r cache=T}
data.path <- paste(project.path, "data/classElicitation-1/", sep = "")

d.catch <- read.csv(paste(data.path, "class-elicitation-full-catch_trials.csv", sep = ""))
d.catch <- d.catch %>%
  mutate(pass = response == "relative to other buildings") %>%
  select(workerid, pass)

d <- read.csv(paste(data.path, "class-elicitation-full-trials.csv", sep = ""))
d.tidy <- left_join(d, d.catch) %>%
  filter(pass) %>%
  mutate(subResponse = ifelse(paraphrase == "super", 0, 1))

df.summary <- d.tidy %>%
  group_by(strength, target, degree, adjective, form, sub_category, super_category) %>%
#  group_by(condition, strength, target, degree, adjective, form, sub_category, super_category) %>%
  multi_boot_standard(column = "subResponse")
```

```{r expt1results, fig.env = "figure*", fig.pos = "t", fig.width=6.6, fig.height=3, fig.align='center', set.cap.width=T, num.cols.cap=2, fig.cap = "Experiment 1 results. Comparison class judgments for 30 items on 5 scales."}


df.summary %>%
  ungroup() %>%
  mutate(sub_category = factor(sub_category, levels = sub_category[order(strength)])#,
          # condition = factor(condition, levels = c("context","contextWithSuper"),
          #                    labels=c("Bare", "Supercat mention"))
         ) %>%
  ggplot(.,
         aes( x = sub_category, y = mean, ymin = ci_lower, ymax = ci_upper, group = form, fill = form ) )+
  geom_bar(stat = 'identity', position = position_dodge(), alpha = 0.8) +
  geom_errorbar(position = position_dodge()) +
  #facet_grid(condition~degree, scales = 'free') +
  facet_grid(.~degree, scales = 'free') +
  ylim(0, 1) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.95) ) +
  scale_fill_brewer(palette = 'Set1') +
  ylab("Proportion subordinate paraphrase") +
  xlab("Item category") +
  theme(axis.title.y = element_text(size = 10),
        strip.text = element_text(size = 8))
```

```{r glm1, cache = T}
rs.glmm <- glmer(data = d.tidy, subResponse ~ form*strength +
                   (1 + form | degree) +
                   (1 + form | sub_category) +
                   (1 | workerid),
                 family = 'binomial')
summ.rs.glmm <- summary(rs.glmm)
```

On each trial, the participant was given an utterance with a gradable adjective and asked which comparison class (subordinate vs. superordinate) the speaker meant.
We observe no systematic differences between participants' responses when the superordinate category was explicitly mentioned previously in the context and those when it was not; thus we collapse across these two conditions for the rest of this paper.
Figure \ref{fig:expt1results} shows the proportion of participants choosing the subordinate category for each item.
We see considerable variability both within- and across- scales.
Within each scale, we see the predicted interaction: For items that are expected to fall high on the scale (basketball players, watermelons, Summer days, dishwashers, movies), positive form adjectives tend to elicit superordinate comparison classes while negative form adjectives tend to elicit subordinate comparison classes (i.e., a basketball player is *tall* relative to other people, but *short* relative to the basketball players).
For items that are expected to fall low on the scale (gymnasts, grapes, Winter days, bottle openers, videos of cute animals), the opposite effect is observed (i.e., a video of a cute animal is *short* relative to other things you watch online, but *long* relative to other videos of cute animals).

This is confirmed using a generalized linear mixed effects model with main effects of adjective form (positive vs. negative) and the *a priori* judgment by the first author of the relative position on the scale of the sub-category (i.e., sub-category item expected to be low, high, or in the middle of the scale), and of critical theoretical interest, the interaction between these two variables
In addition, we included in the model by-participant random effects of intercept, by-degree (temperature, weight, etc...) random effects of intercept and form, and by-subordinate category random effects of intercept and form to predict the probability with which participants would choose the subordinate category as the paraphrase.
\mht{not sure if this is the right test}
As can be seen in Figure \ref{fig:expt1results}, positive form adjectives with *a priori* stronger subordinate categories produce relatively fewer subordinate paraphrases than negative form adjectives $\beta = `r round(summ.rs.glmm[["coefficients"]]["formpositive:strength","Estimate"],2)`$;
$SE = `r round(summ.rs.glmm[["coefficients"]]["formpositive:strength","Std. Error"],2)`;$
$z = `r round(summ.rs.glmm[["coefficients"]]["formpositive:strength","z value"],2)`$.


The two qualitative predictions of our language understanding model were borne out in Experiment 1.
The computational model can in principle make quantitative predictions as well, though these will depend on the quantitative details of the pragmatic listener's world knowledge.
This world knowledge is often measured by having participants estimate relevant quantities assuming only what would be available in the prior [e.g., the temperature in Maryland on a day in Winter; @FrankeEtAl2016].
These quantities can be difficult to estimate, however, especially for abstract superordinate quantities (e.g., the temperature in Maryland on a day of the year).
In the next section, we formalize our uncertainty about listeners' prior knowledge, and run a second experiment to gather converging evidence about listeners' knowledge, which we can then use to generate quantitative predictions for Expt. 1.

# Reducing uncertainty in prior knowledge by asking follow-up questions

Bayesian models of cognition and language strongly rely upon accurate measurement of prior knowledge [@FrankeEtAl2016].
For models of language understanding, prior elicitation tasks typically take the form of running the same language understanding task but removing the target utterance that is aimed to be model [e.g., @Kao2014]. 
Certain quantities and probabilities are inherent difficult measure because they are abstract or hard to estimate.
In previous work, we have attempted to measure prior knowledge by decomposing what would be a single, complicated question into two simpler questions, and then using a Bayesian data analytic approach to reconstruct the prior knowledge [@Tessler2016; @Tessler2016cogsci].
Here, we extend this to ask *natural language* questions that use the same prior knowledge as would be relevant for Expt.~1 and which a reduced form of computational model presented in this paper can answer.
This has the feature of reducing task demand on participants.

As we did for the simulations presented before, we assume the superordinate prior is a unit-normal distribution. 
We assume the subordinate priors also are normal distributions, but with unknown parameters $\mu_{sub}, \sigma_{sub}$.
We can wrap the RSA model presented earlier inside a Bayesian data analysis model and infer these parameters likely values.
Using just the data from Expt.~1, this model would be overparametrized.
We can make a subtle modification of the RSA model to ask follow-up questions that would rely upon the same prior knowledge. 
In particular, we will ask participants that assuming a target entity is a member of the subordinate (e.g., it's a day in Winter), would they predict the adjective would apply with an explicit comparison class (e.g., "Is it warm relative to other days of the year?").

This is a simplification of the comparison class inference model because it eliminates the uncertainty in the relevant comparison class.
Since this question is a felicity-judgment, we model this as a pragmatic speaker [@Qing2014a; @Tessler2016; @Tessler2016cogsci]
\begin{align}
S_{2}(u \mid c_{sub}) &\propto \exp{(\alpha_2 \cdot \ln{L_1(x, \theta \mid u, c_{super})})} \label{eq:S2} \\
L_{1}(x, \theta \mid u, c_{super}) &\propto S_{1}(u \mid x, c_{super}, \theta) \cdot P(x \mid c_{sub}) \cdot P(\theta) \label{eq:L1a}
\end{align}


# Experiment 2: Adjective production

In this experiment, we use a two-alternative forced choice paradigm to collect speaker judgments using the same language model as before.

## Methods

### Participants

We recruited 100 participants from Amazon Mechanical Turk.
Five participants were excluded due to failing the catch trial.
Participation was restricted to those with U.S. IP addresses and who had at least a 95% work approval rating.
On average, the experiment took five minutes and participants were compensated $0.50 for their work. 

### Materials and procedure

Materials were the same as Expt. 1 (Table 1).

Each participant saw 15 trials: one for each subordinate category paired with either the positive or negative form of its corresponding adjective.
Participants never rated the same subordinate category for both adjective forms and back-to-back trials involved different scales to avoid fatigue.
On each trial, participants were given a context sentence to introduce the subordinate category (e.g., *Alicia picks up an apple.*).
This was followed by an adjective sentence, which predicated a positive- or negative-form gradable adjective over the superordinate category of the item (e.g., *Do you think the apple would be light relative to other produce?*).
The experiment can be viewed in full at: 
\url{http://stanford.edu/~mtessler/comparison-class/experiments/vague-prior-elicitation-2afc.html}

## Data analysis and results

```{r, cache = T}
project.name <- "vague-prior-elicitation-1"
data.path <- paste(project.path, "data/vagueSpeaker-1/", sep = "")
d.catch <- read.csv(paste(data.path, project.name, "-catch_trials.csv", sep = ""))
d.catch <- d.catch %>% 
  mutate(pass = response == "Yes")

d <- read.csv(paste(data.path, project.name, "-trials.csv", sep = ""))

vs.summary <- left_join(d, d.catch %>% select(workerid, pass)) %>%
  filter(pass) %>%
  group_by(strength, target, degree, adjective, 
           form, sub_category, super_category) %>%
  multi_boot_standard(column = "response")

```

```{r expt2results, fig.env = "figure*", fig.pos = "t", fig.width=6.6, fig.height=3, fig.align='center', set.cap.width=T, num.cols.cap=2, fig.cap = "Experiment 2 results. Adjective speaker judgments for 30 items on 5 scales."}
vs.summary %>%
  ungroup() %>%
  mutate(sub_category = factor(sub_category, levels = sub_category[order(strength)])) %>%
  ggplot(.,
         aes( x = sub_category, y = mean, ymin = ci_lower, ymax = ci_upper, group = form, fill = form ) )+
  geom_bar(stat = 'identity', position = position_dodge(), alpha = 0.8)+
  geom_errorbar(position = position_dodge())+
  facet_grid(.~degree, scales = 'free')+
  ylim(0, 1) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.95))+
  scale_fill_brewer(palette = 'Set1')+
  ylab("Superordinate adjective endorsement")+
  xlab("Item category")+
  theme(axis.title.y = element_text(size = 10),
        strip.text = element_text(size = 8))
```




# Bayesian data analysis of the Bayesian language model

# Discussion

\begin{enumerate}
\item Speaker knowledge: If you're a basketball scout, and you say of a player that "He is tall." it means "Tall relative to basketball players"
\item QUD: If we're deciding what to do on Friday night, you say "The opera is expensive" it means "Expensive relative to other things we could do on Friday night"
\item Hyperbole / *normative* comparison classs: If we listen to a lecture, and you say "That *was long*.", it means "Long relative to how long I think it should have been." If we go out for pasta, and you ask how it was, and I say "it was expensive" it means "expensive relative to how much it should have been given the quality".
\end{enumerate}

# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.




<!--
## Experiments

See all the items [here](http://stanford.edu/~mtessler/comparison-class/experiments/js/examples-2.js), or see the listener experiment (with all the items) [here](http://stanford.edu/~mtessler/comparison-class/experiments/listener-1.html).

\begin{enumerate}

\item \textbf{Stimuli}
  \subitem Currently we have 40 unique items, made up of 6 scales x 2 (positive and negative form adjectives)
  \subitem Note we are using temperature that is perceived both through touching and temperature that is perceived in the whole body (touching objects vs. the weather). For the weather, it is really about speaker's beliefs--their own subjective comparison class (e.g., John think its cold because he's from Southern California).
  \subitem 3 or 4 contexts by item (different targets of reference)
  \subitem Example: Gary buys a [television, book, iPhone] and picks it up. He says, ``This is heavy.''

\item \textbf{Comparison class elicitation}
  \subitem We have run a pilot using the paraphrase technique with the helper phrases "for a" and "relative to." "For a" produced more reliable results, so we'll use that going forward.
  \subitem Example: Gary buys a television and picks it up. He says, ``This is heavy.''
  \subitem \textbf{Dependent measure}: What do you think Gary meant? ``This is heavy \textbf{for a} ...'' (fill in the blank)
  \subitem \textbf{Data analysis}: Take modal response, or top 2, and use those in the prior elicitation task.

\item \textbf{Prior elicitation}
  \subitem This will refer to the comparison class.
  \subitem \textbf{Dependent measure}: What do you think is the weight of a television? Provide number and units (units from a set from a drop-down menu).

\item \textbf{Language understanding (Listener task)}
  \subitem Same stimuli as the comparison class elicitation.
  \subitem \textbf{Dependent measure}: same as prior elicitation.

\end{enumerate}
# Model

uRSA with lifted variable.
Plug in different priors.

# Data anlaysis

Probably something similar to the habituals paper.
Assume log-normals, do BDA on the prior data.

# Current concerns

\begin{enumerate}

\item In the Justine-way of doing these kinds of experiments, for the prior elicitation, we would have the same context sentence (``Gary buys a television and picks it up.'') without the adjective sentence, and ask about the likely weight.
What does our intermediate comparison class elicitation step buy us?

\item For many of the our items, it seems that the comparison class we're going to get is the class of the target objects (e.g., televisions, cups of coffee, weather in Southern California)... I have the feeling that some items (e.g., a 30 year old who is tall) have more general classes (e.g., tall for an adult, for a woman) while others (e.g., a 4 year old boy who is tall) have more specific classes (i.e., tall for a 4 year old). This example may be unique / idiosyncractic, but also may suggest that the comparison class needs to be a relative homogeneous category (e.g., 4 year olds are really different than 2 year olds, whereas 30 vs. 28 year olds not so much). If this is interesting, should we try to move our stimuli more in that direction? Any thoughts?

\end{enumerate}

-->

# References

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in}
\setlength{\leftskip}{0.125in}
\noindent


<!--
# Outline

1. We introduce an extension to RSA that does inference over what is the comparison class.

2. We take a simple case, where the comparison class is either a sub- or super-ordinate category. Just playing with the parameters off the model, we see this predicted (qualitative) interaction:

- When the subclass has a high mean relative to the superclass, positive form adjectives signal the superclass, and negative form signals the subclass

- When the subclass has a low mean relative to the superclass, positive form adjectives signal the subclass, and negative form signals the superclass

3. We test this predictions on 5 scales (Expt. 1)

- We see the qualitative effect on all 5 scales, but there is considerable heterogeneity among the scales.

4. This heterogeneity might be attributed to differences in the quantitative details (i.e., the parameters) of the subclass vis-a-vis the superclass

- We can perform BDA to see if this is true, but this model is actually overparameterized.

- We can simplify by assuming each superclass has a unit-normal prior, and infer the mean and standard deviation for each subclass prior.

- That’s 2 parameters for each subclass, and we only have 2 items for each subclass (namely, positive and negative form adjectives e.g., “tall” and “short” bball players)

5. We estimate the prior parameters by asking other questions of our model that should (a) access the same priors; and (b) not add other parameters

so we can ask a vague speaker question (Expt 2. [VPE])

This will alleviate the overparameterization problem and is, in general, a new way of testing language understanding models without having to explicitly measure priors
-->
