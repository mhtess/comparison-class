\documentclass[doc]{apa6}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{apacite}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{tikz}
\usetikzlibrary{bayesnet}

%\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
%\DeclareLanguageMapping{american}{american-apa}

  \title{Supplemental Information: Big \emph{for a what}? Inferring comparison classes for relative adjectives}
    \author{Michael Henry Tessler and Noah D. Goodman}
    \date{}
  
\shorttitle{ SI: Inferring comparison classes}
\affiliation{Stanford University}
\keywords{comparison class; pragmatics; Rational Speech Act; Bayesian cognitive model; Bayesian data analysis\newline\indent Word count: X}
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage{gensymb}
\usepackage{tikz}
\usepackage{caption}
\usepackage{booktabs}

\begin{document}
\maketitle

\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}
\definecolor{Orange}{RGB}{255,153,0}

\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\red}[1]{\textcolor{Red}{#1}}  
\newcommand{\ndg}[1]{\textcolor{Green}{[ndg: #1]}}  
\newcommand{\mht}[1]{\textcolor{Blue}{[mht: #1]}}  
\newcommand{\mlb}[1]{\textcolor{Orange}{[mlb: #1]}}

\section{Bayesian Data Analysis}

The model's quantitative predictions can be generated by explicitly specifying the interlocutors' relevant prior knowledge (e.g., beliefs about temperatures). 
The current methodological standard is to measure prior beliefs empirically by creating tasks for participants to estimate quantities or give likelihood judgments \cite{Franke2016}. 
We pursue a different methodology. 
The RSA model captures a productive fragment of natural language; thus, it makes predictions about a related natural language task.
Critically, we can use the model to predict natural language judgments that require the \emph{same prior knowledge} as in Expt. 1 and use Bayesian data analysis to jointly infer the shared priors. 
This approach harnesses the productivity of language into experiment design and allows us to reconstruct priors without having participants engage in challenging numerical estimation tasks.

\subsection{Overview of data analytic approach}

The behavioral experiment bore out our two qualitative predictions
derived from the comparison class inference model. The model is a
quantitative model, however, and can make quantitative predictions
concerning the strength of comparison class inferences. Indeed, we
observe substantial variability in the predicted inferences both within-
and across- scales. Here we test whether the observed variability in
inferences can be accounted for by the constructs posited in our model.
We describe the two relevant constructs---the comparison class prior
\(P(c)\) and the degree priors \(P(x \mid c)\)---before outlining our
Bayesian data analytic strategy.

\subsubsection{Comparison class prior}

\(P(c)\) reflects listeners' expectations of what classes are likely to be discussed. As a proxy for comparison class usage frequency, we use empirical frequency \(\hat{f}\) estimated from the Google WebGram corpus\footnote{Corpus accessed via
\url{https://corpora.linguistik.uni-erlangen.de/cgi-bin/demos/Web1T5/Web1T5_freq.perl}. 
Due to potential polysemy and idiosyncracies of our experimental
  materials (Table 1), we made the following substitutions when querying
  the database for empirical frequency: produce $\rightarrow$
  ``fruits and vegetables''; things you watch online
  $\rightarrow$ ``online videos''; days in \{season\}
  $\rightarrow$ ``\{season\} days''; dishwashers $\rightarrow$
  ``dishwashing machines''; videos of cute animals $\rightarrow$
  ``animal videos''.}, and scale it by a free parameter $\beta$ such that $P(c) \propto \exp{(\beta \cdot \log \hat{f})}$.

\subsubsection{Degree priors (World knowledge)}

Only the relative values for \(P(x \mid c_{sub})\) and \(P(x \mid c_{super})\) affect model predictions. 
Hence we fix each superordinate distribution to be a standard normal distribution \(P(x \mid c_{super}) = \mathcal{N}(0, 1)\) and the subordinate priors to also be Gaussian distributions \(P(x \mid c_{sub}) = \mathcal{N}(\mu_{sub}, \sigma_{sub})\); the subordinate priors thus have standardized units.

Specifying the relevant prior knowledge yields two free parameters per subordinate class. We put priors over these parameters and infer their likely values using Bayesian data analysis. The data from the comparison class inference experiment would be insufficient, however, to reliably estimate all of the parameters of this data analytic model. To alleviate this, we use the same RSA model to predict additional data about related language used in the same domains. Specifically, we gather judgments about adjectives when the comparison class is explicit (\(n = 100\)): whether or not an adjective would apply to a subordinate member explicitly relative to the superordinate category (e.g., Is a day in winter warm relative to other days of the year?).\footnote{The judgments in this experiment were consistent with the \emph{a priori} ordering of the subordinate categories on the degree scale (e.g., a day in winter is likely to be cold for the year).}

To model this adjective endorsement data, we remove comparison class uncertainty by setting \(P(c_{super}) = 1\), since the sentence being endorsed include an explicit comparison to the superordinate class. We model sentence endorsement using a pragmatic speaker (following Qing \&Franke, 2014a; Tessler \& Goodman, 2016a, 2016b): \vspace{-0.5cm}
\begin{equation}
S_{2}(u \mid c_{sub}) \propto \exp{(\alpha_2 \cdot {\mathbb E}_{x\sim P_{c_{sub}}} \ln{L_1(x \mid u)})} \label{eq:S2a}
\end{equation} 

\noindent Note that $L_1(x \mid u)$ is defined from Eq. \ref{eq:L1a} by marginalization.

Eqs. \ref{eq:L1a} and \ref{eq:S2} define models for the comparison class inference task and adjective endorsement task, respectively, and depend on the same background knowledge \(P(x\mid c)\). We can thus use data from both experiments to jointly reconstruct the shared prior knowledge and generate predictions for the two data sets.

\section{Full model analysis and results}

The RSA listener (Eq. \ref{eq:L1a}) and speaker (Eq. \ref{eq:S2}) models make quantitative predictions about comparison class interpretation and adjective endorsement, respectively. We construct a single data-analytic model with each of these RSA components as sub-models in order to make quantitative predictions about the data from both of our experiments.

The listener and speaker sub-models share their prior world knowledge \(P(x \mid c)\) (e.g., temperatures in winter), described in the \textbf{Degree Priors} section. We put the same priors over the parameters of each subordinate distribution:
\(\mu \sim \text{Uniform}(-3, 3)\), \(\sigma \sim \text{Uniform}(0, 5)\), since they have standardized units. The comparison class prior \(P(c)\) in Eq. \ref{eq:L1a} scales the empirical frequency \(\hat{f}\) by a free parameter, which we give the following prior: \(\beta \sim \text{Uniform}(0, 3)\).

The full model has three additional parameters not of direct theoretical interest: the speaker optimality parameters \(\alpha^\text{expt}_{i}\), which can vary across the two tasks. The pragmatic listener \(L_1\) model (Eq. \ref{eq:L1a}) has one speaker optimality: \(\alpha^\text{1}_{1}\). The pragmatic speaker \(S_2\) model (Eq. \ref{eq:S2}) has two speaker optimality parameters: \(\{\alpha^\text{2}_{1}, \alpha^\text{2}_{2}\}\). We use priors consistent with the previous literature: \(\alpha_1 \sim \text{Uniform}(0, 20)\), \(\alpha_2 \sim \text{Uniform}(0, 5)\)

We implemented the RSA and Bayesian data analysis models in the probabilistic programming language WebPPL (Goodman \& Stuhlmuller, 2014). To learn about the credible values of the parameters, we collected 2 chains of 50k iterations (after 25k burn-in) using an incrementalized version of MCMC (Ritchie, Stuhlmuller, \& Goodman, 2016).

\subsubsection{Results}

The full model's posterior over the RSA and data-analytic parameters were consistent with prior literature and intuition. The maximum a-posteriori (MAP) estimate and 95\% highest probability density (HPD) intervals for model parameters specific to the \(L_1\) model used for comparison class inference were \(\alpha^{1}_{1} = 1.60 [1.10, 2.50]\),
\(\beta = 0.13 [0.11, 0.19]\). Model parameters specific to the \(S_2\) model used for adjective endorsement:
\(\alpha^{2}_{1} = 3.50 [0.60, 13.20]\), \(\alpha^{2}_{2} = 3.20 [2.60, 3.80]\). The inferred distributions corresponding to subordinate class priors were consistent with the \emph{a priori} ordering of these subordinate classes (low, medium, high) used in these tasks (Figure \ref{fig:modelParameters} top).

Finally, the full model's posterior predictive distribution does an excellent job at capturing the quantitative variability in comparison class inferences: \(r^2(30) = 0.96\), and adjective endorsements: \(r^2(30) = 0.98\) (Figure \ref{fig:posteriorPredictiveScatters}). Because of the overall preference for the subordinate comparison class, many of the data points are distributed above 0.5. Even for these fine-grained differences, the model does a good job at explaining the quantitative variability in participants' data (Figure \ref{fig:posteriorPredictiveScatters} right). Thus, the variability in comparison class inferences we observe in our behavioral data can be accounted for the constructs posited in our model (namely, the comparison class prior and degree priors).

\subsection{Fully Bayesian analysis of Bayesian language
models}

The second contribution of this paper is a novel data-analytic approach,
where prior knowledge used in the Bayesian language model is
reconstructed from converging evidence gathered from a related language
experiment, also explicitly modeled using a language understanding
model. In previous work, we have attempted to measure prior knowledge by
decomposing what would be a single, implicitly multilayered, numerical
estimation question into multiple simpler questions. Then, we construct
a Bayesian data analytic model to back out the prior knowledge (Tessler
\& Goodman, 2016a, 2016b). We extend this approach by using the same
core RSA model to model behavior across two language experiments. The
major feature of this method is that participants respond only to
simple, natural language questions rather than estimating numerical
quantities for which complicated linking functions must be designed
(e.g., Franke et al., 2016). The fully Bayesian language approach we
pioneer here also provides a further constraint on the language model,
which must predict data from two similar but distinct language
experiments. The productivity of natural language can thus be harnessed
to productively design experiments that further constrain and test
computational models of language and cognition.


\newpage


\bibliographystyle{apacite}

%\setlength{\bibleftmargin}{.125in}
%\setlength{\bibindent}{-\bibleftmargin}

\bibliography{comparison-class}

\end{document}
